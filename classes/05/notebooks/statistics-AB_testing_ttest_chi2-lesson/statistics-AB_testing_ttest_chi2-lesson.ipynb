{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## DS-SF-42 | 05 | Hypothesis Testing\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the fundamental concepts of Frequentist hypothesis testing\n",
    "- Understand the difference between the null and alternative hypothesis\n",
    "- Apply an understanding of statistical hypothesis testing within the context of split testing.\n",
    "- Apply the independent samples t-test\n",
    "- Apply the chi-squared test of independence to \"winner\" a split test.\n",
    "- Understand the relationship between p-values, alpha thresholds, and statistical significance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a T-test in Python (with a discussion of Type II error)\n",
    "\n",
    "---\n",
    "\n",
    "Recall that Z-test only works when you know the population mean and standard devation, which is rarely the case. Often it's only possible to compare two independent samples. The most basic scenario is one control and one treatment group. If the outcome between the two is sufficiently large, then it is reasonable to deem the null-hypothesis impropable enough to reject.\n",
    "\n",
    "**EXAMPLE:** _Are DS part-time students taller than DSI students?_\n",
    "\n",
    "We might collect data on two classes, of thirty students each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate data\n",
    "fulltime = np.array([69., 69., 63., 65., 65., 67., 59., 63., 70., 59., 67., 64., 72.,\n",
    "                    70., 65., 65., 65., 64., 65., 66., 63., 63., 62., 66., 62., 66.,\n",
    "                    66., 64., 71., 68.])\n",
    "\n",
    "parttime = np.array([74., 72., 61., 65., 65., 70., 64., 66., 65., 67., 67., 69., 73.,\n",
    "                     73., 71., 66., 65., 76., 63., 67., 67., 69., 61., 62., 62., 67.,\n",
    "                     59., 66., 62., 68.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### <font color=blue>Partner Practice</font> - Activity: What do you make of these data? Is there a real difference? How certain can you be? Use descriptive statistics, plotting, or any other tools you think might be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Investigate here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's leverage inferential statistics.\n",
    "\n",
    "### The \"null hypothesis\"\n",
    "\n",
    "---\n",
    "\n",
    "The **null hypothesis** is a fundamental concept of Frequentist statistical tests. We typically denote the null hypothesis with **H0**. \n",
    "\n",
    "In our GA height example, we can define our null hypothesis to be that there is no difference in mean height between part-time and full-time student populations.\n",
    "\n",
    "> **H0:** The mean difference between groups is zero.\n",
    "\n",
    "### The \"alternative hypothesis\"\n",
    "\n",
    "---\n",
    "\n",
    "The **alternative hypothesis** is the outcome of the experiment that we hope to show. In our example the alternative hypothesis is that there is in fact a mean difference in height. \n",
    "\n",
    "> **H1:** The parameter of interest, our mean difference between groups, is not zero.\n",
    "\n",
    "**NOTE:** The null hypothesis and alternative hypothesis are concerned with the true values, or in other words the *parameter of the overall population*. Through the process of experimentation / hypothesis testing and statistical analysis of the results we will make an *inference* about this population parameter.\n",
    "\n",
    "#### In our case:\n",
    "\n",
    "H1: There is true difference in means between the heights of parttime and fulltime DS students at GA.\n",
    "\n",
    "H0: There is no mean difference between heights between parttime and fulltime students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Performing a t-test using scipy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Check - </font> How do we interpret the p-value from this t-test?\n",
    "\n",
    "### <font color=blue>Discussion - </font> When I generated the data, I deliberately sampled it from two normal distributions with _different_ means. The null hypothesis was incorrect. Why couldn't we reject it?\n",
    "\n",
    "### Which elements of an experiment minimize Type II error?\n",
    "- \n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab-testing'></a>\n",
    "\n",
    "### Introduction to A/B testing\n",
    "\n",
    "---\n",
    "\n",
    "You may have heard the term \"A/B testing,\" or \"split testing,\" before. Simply put, a split test is an experiment that tests different versions of your product with your users. Using these results, you as the data scientist will statistically analyze the experiment and determine a \"winner\" according to a pre-defined metric. \n",
    "\n",
    "**Example: Selling dog collars**\n",
    "\n",
    "Picture this: You work for a startup that sells dog collars. Your web development team has constructed a prototype for a new \"landing page\" on the website (a landing page is the first page users reach when visiting a site). The designers are not sure whether a picture of a black lab wearing the collar or a golden retriever wearing the collar will have more of an impact on the click-through rate (the proportion of users who continue on to the the rest of the website). \n",
    "\n",
    "The team decides to run an A/B test to quantitatively evaluate which picture to choose.\n",
    "- **Arm A** is the version of the landing page with the black lab.\n",
    "- **Arm B** is the version of the landing page with the golden retriever.\n",
    "\n",
    "For two weeks, users will be directed at random to one of the two landing pages with equal probability. At the end of this period, the click-through rates of each arm will be compared and one of the two will be \"winnered.\"\n",
    "\n",
    "Desiging and evaluating A/B tests like this one is one of the most common tasks a data scientist will be asked to perform.\n",
    "\n",
    "**Below are the click-through rates per arm, measured at the end of two weeks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_wins = 115\n",
    "A_loss = 85\n",
    "B_wins = 87\n",
    "B_loss = 103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab-hypothesis'></a>\n",
    "\n",
    "### Split Tests Are Hypothesis Tests\n",
    "\n",
    "---\n",
    "\n",
    "Despite the business jargon, **split tests are just experiments to test hypotheses.** Using the scenario above, we can frame the null hypothesis like so:\n",
    "\n",
    "> **H0:** The difference in click-through rates between arms is 0.\n",
    "\n",
    "The alternative hypothesis would be:\n",
    "\n",
    "> **H1:** The difference in click-through rates between arms is not 0.\n",
    "\n",
    "It's important that the users sent to each arm are selected at random. If user assignments are affected by external factors — such as whether they are viewing the site on web or mobile browsers — then the arms have **selection bias**.\n",
    "\n",
    "**What is the problem with choosing a picture if users were not randomly assigned?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chisq'></a>\n",
    "\n",
    "### The $\\chi^2$ (Chi-Squared) Test of Independence\n",
    "\n",
    "---\n",
    "\n",
    "A popular Frequentist method for evaluating A/B tests is the $\\chi^2$ test of independence. The $\\chi^2$ test of independence is appropriate when you have categorical data and want to evaluate whether or not two groups are significantly different. \n",
    "\n",
    "Click-through rate can be thought of as binary categorical data: A user either clicked through (1) or did not (0). \n",
    "\n",
    "\"Independence\" refers to whether or not the outcome for the groups (the click-through rate) is independent of group assignment. Independence would mean that there is no relationship between the dog picture and the click-through rate. \n",
    "\n",
    "You can conduct the $\\chi^2$ test manually using what is known as a contingency table. For a detailed overview of the procedure, [this site](https://onlinecourses.science.psu.edu/stat500/node/56) is a good resource. In this course, we will use Python instead of manual calculation. That being said, it is important to address the formula for the $\\chi^2$ statistic:\n",
    "\n",
    "### $$ \\chi^2 = \\sum_{i=1}^{cells} \\frac{(O_i - E_i)^2}{E_i} $$\n",
    "\n",
    "Where: \n",
    "\n",
    "- $cells$ refers to the number of cells in the contingency table.\n",
    "- $O$ are the observed values (frequencies).\n",
    "- $E$ are the *expected* frequencies under perfect independence. \n",
    "\n",
    "**Using `stats.chi2_contingency`, calculate the $\\chi^2$ statistic and the associated p-value for our split test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain what the p-value means in the context of our split test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which arm is the \"winner\"? Should you choose to accept it as such? By how much?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='significance'></a>\n",
    "\n",
    "### Statistical Significance, P-Values, The Alpha Threshold, and Type I Errors\n",
    "\n",
    "---\n",
    "\n",
    "The split test has concluded and you have performed your statistical analysis of the results.\n",
    "- **Arm A (the black lab picture) has a 23% higher click-through rate than Arm B (the golden retriever picture)**.\n",
    "- **The p-value of our test was 0.044**.\n",
    "\n",
    "Should we accept that the black lab picture is in fact more effective? Do we believe that the difference is real?\n",
    "\n",
    "**Statistical Significance**\n",
    "\n",
    "It's common to see Frequentist tests reported as \"statistically significant with p < 0.05\" or \"with p < 0.01.\" So, what does it mean for a test to be statistically significant? On the surface, these statements are simply saying that the calculated p-value is less than a specific value. The values of 0.05 and 0.01 are common in academic research but also arbitrary. \n",
    "\n",
    "What does having \"p < 0.05\" specifically mean?\n",
    "\n",
    "> **p < 0.05**: In hypothetical repetitions of this experiment with the same sample size, fewer than 5% of the experiments would have measured a difference between arms at least this extreme _by chance_.\n",
    "\n",
    "The same goes for \"p < 0.01.\" Here, however, it's framed in the context of null and alternative hypotheses:\n",
    "\n",
    "> **p < 0.01**: There is less than a 1% chance of accepting the alternative hypothesis when the null hypothesis is in fact true. \n",
    "\n",
    "---\n",
    "\n",
    "**Type I Errors and the Alpha Threshold ($\\alpha$**)\n",
    "\n",
    "As rigorous researchers, we would set a threshold for how likely we are to falsely accept the alternative hypothesis prior to running our experiment. This chance is known as a **type I error**.\n",
    "\n",
    "Type I errors are directly related to the p-value. \n",
    "\n",
    "> **The p-value represents the risk of encountering a type I error, given the sample size and measured effect.**\n",
    "\n",
    "It is important to set thresholds for type I errors before experiments begin. This prevents us from arbitrarily deciding whether or not we will accept an alternative hypothesis after we see a p-value.\n",
    "\n",
    "The threshold we set for the type I error rate is denoted with `alpha`. For example:\n",
    "\n",
    "- $\\alpha$ = 0.05 corresponds to a \"p < 0.05\" significance level.\n",
    "- $\\alpha$ = 0.01 corresponds to a \"p < 0.01\" significance level.\n",
    "\n",
    "**A Side Note on P-Value Thresholds:** \n",
    "\n",
    "\"p < 0.01\" is historically considered a \"conservative\" significance threshold. But it's actually not very conservative at all. \n",
    "\n",
    "This is, at worst, a 1/100 chance that an alternative hypothesis will be accepted when the result is in fact null. Now, think about all of the papers written and experiments run that have used a threshold of \"p < 0.01\" to validate their findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "----\n",
    "- Frequentists frame an experimental question in terms of a null hypothesis and an alternative hypothesis.\n",
    "    - The alternative hypothesis is the result you aim to support\n",
    "    - The null hypothesis states the absence of an effect\n",
    "- Use a t-test, or an ANOVA(F-test) for continuous data\n",
    "- Use a chi-2 test of independence for for frequencies of categorical data\n",
    "- We can implement hypothesis tests using `scipy.stats`\n",
    "- When running a hypothesis test you run the risk of two types of errors:\n",
    " - **Type I**: Incorrectly reject the null hypothesis (false positive)\n",
    "     - Alpha threshold is too high\n",
    "     - Too many repeated experiments\n",
    " - **Type II**: Fail to reject the null hypothesis when it actually is false (false negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds42)",
   "language": "python",
   "name": "ds42"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
