{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "## Type I Error, Type II Error, and Power Analysis\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Apply an understanding of statistical hypothesis testing within the context of split testing.\n",
    "- Apply the chi-squared test of independence to \"winner\" a split test.\n",
    "- Understand the relationship between p-values, alpha thresholds, and statistical significance.\n",
    "- Understand the difference between a type I error, statistical power, and a type II error.\n",
    "- Visualize the interaction of `alpha` and `power`.\n",
    "- Understand how the components of experimental design interact.\n",
    "- Learn how to calculate the statistical power of a test.\n",
    "- Learn how to calculate the required sample size of a test.\n",
    "- Visualize the required sample size of a test, given its other requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Introduction to A/B Testing](#ab-testing)\n",
    "- [Split Tests are Hypothesis Tests](#ab-hypothesis)\n",
    "- [Chi-Squared Test of Independence](#chisq)\n",
    "- [Statistical Significance, P-Values, The Alpha Threshold, and Type I Errors](#significance)\n",
    "- [Type II Errors and Statistical Power](#type2-power)\n",
    "- [Visualizing the Interaction Between `alpha` and `power`](#power-visual)\n",
    "- [Power Analysis and Sample Size Calculation](#power-analysis)\n",
    "- [Calculating the Required Sample Size Visually](#visual-equation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab-testing'></a>\n",
    "\n",
    "### Introduction to A/B testing\n",
    "\n",
    "---\n",
    "\n",
    "You may have heard the term \"A/B testing,\" or \"split testing,\" before. Simply put, a split test is an experiment that tests different versions of your product with your users. Using these results, you as the data scientist will statistically analyze the experiment and determine a \"winner\" according to a pre-defined metric. \n",
    "\n",
    "**Example: Selling dog collars**\n",
    "\n",
    "Picture this: You work for a startup that sells dog collars. Your web development team has constructed a prototype for a new \"landing page\" on the website (a landing page is the first page users reach when visiting a site). The designers are not sure whether a picture of a black lab wearing the collar or a golden retriever wearing the collar will have more of an impact on the click-through rate (the proportion of users who continue on to the the rest of the website). \n",
    "\n",
    "The team decides to run an A/B test to quantitatively evaluate which picture to choose.\n",
    "- **Arm A** is the version of the landing page with the black lab.\n",
    "- **Arm B** is the version of the landing page with the golden retriever.\n",
    "\n",
    "For two weeks, users will be directed at random to one of the two landing pages with equal probability. At the end of this period, the click-through rates of each arm will be compared and one of the two will be \"winnered.\"\n",
    "\n",
    "Desiging and evaluating A/B tests like this one is one of the most common tasks a data scientist will be asked to perform.\n",
    "\n",
    "**Below are the click-through rates per arm, measured at the end of two weeks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A_wins = 113\n",
    "A_loss = 87\n",
    "B_wins = 87\n",
    "B_loss = 103"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab-hypothesis'></a>\n",
    "\n",
    "### Split Tests Are Hypothesis Tests\n",
    "\n",
    "---\n",
    "\n",
    "Despite the business jargon, **split tests are just experiments to test hypotheses.** Using the scenario above, we can frame the null hypothesis like so:\n",
    "\n",
    "> **H0:** The difference in click-through rates between arms is 0.\n",
    "\n",
    "The alternative hypothesis would be:\n",
    "\n",
    "> **H1:** The difference in click-through rates between arms is not 0.\n",
    "\n",
    "It's important that the users sent to each arm are selected at random. If user assignments are affected by external factors — such as whether they are viewing the site on web or mobile browsers — then the arms have **selection bias**.\n",
    "\n",
    "**What is the problem with choosing a picture if users were not randomly assigned?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='chisq'></a>\n",
    "\n",
    "### The $\\chi^2$ (Chi-Squared) Test of Independence\n",
    "\n",
    "---\n",
    "\n",
    "A popular Frequentist method for evaluating A/B tests is the $\\chi^2$ test of independence. The $\\chi^2$ test of independence is appropriate when you have categorical data and want to evaluate whether or not two groups are significantly different. \n",
    "\n",
    "Click-through rate can be thought of as binary categorical data: A user either clicked through (1) or did not (0). \n",
    "\n",
    "\"Independence\" refers to whether or not the outcome for the groups (the click-through rate) is independent of group assignment. Independence would mean that there is no relationship between the dog picture and the click-through rate. \n",
    "\n",
    "You can conduct the $\\chi^2$ test manually using what is known as a contingency table. For a detailed overview of the procedure, [this site](https://onlinecourses.science.psu.edu/stat500/node/56) is a good resource. In this course, we will use Python instead of manual calculation. That being said, it is important to address the formula for the $\\chi^2$ statistic:\n",
    "\n",
    "### $$ \\chi^2 = \\sum_{i=1}^{cells} \\frac{(O_i - E_i)^2}{E_i} $$\n",
    "\n",
    "Where: \n",
    "\n",
    "- `$cells$` refers to the number of cells in the contingency table.\n",
    "- `$O$` are the observed values (frequencies).\n",
    "- `$E$` are the *expected* frequencies under perfect independence. \n",
    "\n",
    "**Using `stats.chi2_contingency`, calculate the `$\\chi^2$` statistic and the associated p-value for our split test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.05546658587 0.0440285500395\n"
     ]
    }
   ],
   "source": [
    "table = np.array([[A_wins, A_loss],\n",
    "                  [B_wins, B_loss]])\n",
    "\n",
    "results = stats.chi2_contingency(table)\n",
    "chi2 = results[0]\n",
    "pvalue = results[1]\n",
    "print chi2, pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explain what the p-value means in the context of our split test.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_We have a p-value of about 0.044. This means that there is a 4.4% chance that we observed the difference in click-through rates between arms because of randomness in our sample when, in fact, there is no difference in the overall population. (That is, if everyone in the world were to visit our site)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which arm is the \"winner\"? Should you choose to accept it as such? By how much?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.565 0.457894736842\n",
      "0.107105263158\n",
      "1.23390804598\n"
     ]
    }
   ],
   "source": [
    "# Arm A has the higher click-through rate and would be the winner if this level\n",
    "# of significance were sufficient.\n",
    "\n",
    "A_rate = float(A_wins)/(A_wins+A_loss)\n",
    "B_rate = float(B_wins)/(B_wins+B_loss)\n",
    "A_vs_B = A_rate - B_rate\n",
    "print A_rate, B_rate\n",
    "print A_vs_B\n",
    "print A_rate/B_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='significance'></a>\n",
    "\n",
    "### Statistical Significance, P-Values, The Alpha Threshold, and Type I Errors\n",
    "\n",
    "---\n",
    "\n",
    "The split test has concluded and you have performed your statistical analysis of the results.\n",
    "- **Arm A (the black lab picture) has a 23% higher click-through rate than Arm B (the golden retriever picture)**.\n",
    "- **The p-value of our test was 0.044**.\n",
    "\n",
    "Should we accept that the black lab picture is in fact more effective? Do we believe that the difference is real?\n",
    "\n",
    "**Statistical Significance**\n",
    "\n",
    "It's common to see Frequentist tests reported as \"statistically significant with p < 0.05\" or \"with p < 0.01.\" So, what does it mean for a test to be statistically significant? On the surface, these statements are simply saying that the calculated p-value is less than a specific value. The values of 0.05 and 0.01 are common in academic research but also arbitrary. \n",
    "\n",
    "What does having \"p < 0.05\" specifically mean?\n",
    "\n",
    "> **p < 0.05**: In hypothetical repetitions of this experiment with the same sample size, fewer than 5% of the experiments would have measured a difference between arms at least this extreme _by chance_.\n",
    "\n",
    "The same goes for \"p < 0.01.\" Here, however, it's framed in the context of null and alternative hypotheses:\n",
    "\n",
    "> **p < 0.01**: There is less than a 1% chance of accepting the alternative hypothesis when the null hypothesis is in fact true. \n",
    "\n",
    "---\n",
    "\n",
    "**Type I Errors and the Alpha Threshold ($\\alpha$**)\n",
    "\n",
    "As rigorous researchers, we would set a threshold for how likely we are to falsely accept the alternative hypothesis prior to running our experiment. This chance is known as a **type I error**.\n",
    "\n",
    "Type I errors are directly related to the p-value. \n",
    "\n",
    "> **The p-value represents the risk of encountering a type I error, given the sample size and measured effect.**\n",
    "\n",
    "It is important to set thresholds for type I errors before experiments begin. This prevents us from arbitrarily deciding whether or not we will accept an alternative hypothesis after we see a p-value.\n",
    "\n",
    "The threshold we set for the type I error rate is denoted with `alpha`. For example:\n",
    "\n",
    "- $\\alpha$ = 0.05 corresponds to a \"p < 0.05\" significance level.\n",
    "- $\\alpha$ = 0.01 corresponds to a \"p < 0.01\" significance level.\n",
    "\n",
    "Furthermore, `alpha` is equivalent to `1.0 - confidence`. A 95% confidence interval, for example, is associated with a type I error threshold of \"$\\alpha$ = 0.05.\"\n",
    "\n",
    "**A Side Note on P-Value Thresholds:** \n",
    "\n",
    "\"p < 0.01\" is historically considered a \"conservative\" significance threshold. But it's actually not very conservative at all. \n",
    "\n",
    "This is, at worst, a 1/100 chance that an alternative hypothesis will be accepted when the result is in fact null. Now, think about all of the papers written and experiments run that have used a threshold of \"p < 0.01\" to validate their findings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds42)",
   "language": "python",
   "name": "ds42"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
