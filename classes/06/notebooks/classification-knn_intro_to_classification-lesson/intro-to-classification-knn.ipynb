{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Classification with K-Nearest Neighbors\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Alexander Barriga (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the difference between classification and regression models\n",
    "- Understand the K-Nearest Neighbors algorithm visually and in pseudocode\n",
    "- Explain the differences between distance metrics and explore the two most common\n",
    "- Apply KNN classification to the Wisconsin breast cancer dataset\n",
    "- Practice manually performing stratified cross-validation\n",
    "- Visually examine the effect of K neighbors on the decision boundary\n",
    "- Explain the effect of choosing K on the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction: regression vs. classification\n",
    "\n",
    "We've discussed the difference between continuous and discrete numbers. That difference is what distinguishes classification from regression prediction tasks. Today we are going to focus on predicting non-quantitative, discrete categories, which is known as classification.\n",
    "\n",
    "Take wine for example. You could predict a quality rating using regression, but what if we just wanted to predict whether wine was good or bad? Red or white? \n",
    "\n",
    "Classification algorithms do just that; they predict categories, or classes. Split the data into groups and place new data into those groups. \n",
    "\n",
    "![](http://ipython-books.github.io/images/ml.png \"Best Split vs Best Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression**\n",
    "- Either continuous or categorical predictors\n",
    "- Continuous target\n",
    "\n",
    "**Classification**\n",
    "- Either continuous or categorical predictors\n",
    "- Categorical target\n",
    "\n",
    "KNN - hyperparameter --> something you can turn on and off\n",
    "- K determines how big the area you are looking at"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn-visual-intro'></a>\n",
    "\n",
    "---\n",
    "\n",
    "### K-Nearest Neighbors (KNN) visually\n",
    "\n",
    "**KNN works similarly to how we humans might choose to classify things. Below we have some red and blue dots:**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_reds_and_blues.png \"Some Dots\")\n",
    "\n",
    "**A new dot appears without a color and we need to decide which color it is most likely going to be.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point.png \"A New Dot Appears\")\n",
    "\n",
    "**We compare it to its three nearest neighbors â€“ its neighbors are more often red, so we label it red.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred.png \"3 Nearest Neighbors\")\n",
    "\n",
    "**What if we increase the number of neighbors to consider to 5?**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred_blue.png \"5 Nearest Neighbors\")\n",
    "\n",
    "**This is in essence the K-Nearest Neighbors (KNN) algorithm. The K represents the number of \"neighbors\" you use.**\n",
    "\n",
    "> ***Images above credited to the yhat blog.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>\n",
    "\n",
    "## The KNN algorithm\n",
    "\n",
    "---\n",
    "\n",
    "K-Nearest Neighbors takes a unique approach to finding patterns in the data. In order to estimate a value (regression) or class membership (classification), the algorithm finds the observations in its training data that are \"nearest\" to the observation to predict. It then takes a vote of those training observations' target values to estimate the value for the new data point.\n",
    "\n",
    "Distance is usually calculated using the euclidean distance. The \"K\" in KNN refers to the number of nearest neighbors that will be contributing to the prediction. \n",
    "\n",
    "Today we will be looking at KNN only in the context of classification.\n",
    "\n",
    "**The KNN can be concisely represented with pseudocode:**\n",
    "\n",
    "```\n",
    "for unclassified_point in sample:\n",
    "    for known_point in known_class_points:\n",
    "        calculate distances (euclidean or other) between known_point and unclassified_point\n",
    "    for k in range of specified_neighbors_number:\n",
    "        find k_nearest_points in known_class_points to unclassified_point\n",
    "    assign class to unclassified_point using \"votes\" from k_nearest_points\n",
    "```\n",
    "\n",
    "> **Note**: in the case of ties, sklearn's `KNeighborsClassifier()` will just choose the first class (when weights are uniform)! If this is unappealing to you you can change the weights keyword argument to 'distance'. More on this later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='distance'></a>\n",
    "## The KNN distance metric\n",
    "\n",
    "---\n",
    "KNN typically uses one of two distance metrics: euclidean or manhattan. Other distance metrics are possible, but more rare (sometimes it makes sense to create your own distance function.\n",
    "\n",
    "<a id='euclidean'></a>\n",
    "### Euclidean distance\n",
    "\n",
    "Recall the famous Pythagorean Theorem\n",
    "![Alt text](http://ncalculators.com/images/pythagoras-theorem.gif)\n",
    "\n",
    "We can apply the theorem to calculate distance between points. This is called Euclidean distance. \n",
    "\n",
    "![Alt text](http://rosalind.info/media/Euclidean_distance.png)\n",
    "\n",
    "### $$\\text{Euclidean  distance}=\\sqrt{(x_1-x_2)^2+(y_1-y_1)^2}$$\n",
    "\n",
    "There are many different distance metrics, but Euclidean is the most common (and default in sklearn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wisconsin'></a>\n",
    "\n",
    "## Load the wine dataset\n",
    "\n",
    "---\n",
    "\n",
    "Below we will be testing out the KNN classification algorithm on the classic [UCI Wine Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!echo \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DESCR': 'Wine Data Database\\n====================\\n\\nNotes\\n-----\\nData Set Characteristics:\\n    :Number of Instances: 178 (50 in each of three classes)\\n    :Number of Attributes: 13 numeric, predictive attributes and the class\\n    :Attribute Information:\\n \\t\\t- 1) Alcohol\\n \\t\\t- 2) Malic acid\\n \\t\\t- 3) Ash\\n\\t\\t- 4) Alcalinity of ash  \\n \\t\\t- 5) Magnesium\\n\\t\\t- 6) Total phenols\\n \\t\\t- 7) Flavanoids\\n \\t\\t- 8) Nonflavanoid phenols\\n \\t\\t- 9) Proanthocyanins\\n\\t\\t- 10)Color intensity\\n \\t\\t- 11)Hue\\n \\t\\t- 12)OD280/OD315 of diluted wines\\n \\t\\t- 13)Proline\\n        \\t- class:\\n                - class_0\\n                - class_1\\n                - class_2\\n\\t\\t\\n    :Summary Statistics:\\n    \\n    ============================= ==== ===== ======= =====\\n                                   Min   Max   Mean     SD\\n    ============================= ==== ===== ======= =====\\n    Alcohol:                      11.0  14.8    13.0   0.8\\n    Malic Acid:                   0.74  5.80    2.34  1.12\\n    Ash:                          1.36  3.23    2.36  0.27\\n    Alcalinity of Ash:            10.6  30.0    19.5   3.3\\n    Magnesium:                    70.0 162.0    99.7  14.3\\n    Total Phenols:                0.98  3.88    2.29  0.63\\n    Flavanoids:                   0.34  5.08    2.03  1.00\\n    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\\n    Proanthocyanins:              0.41  3.58    1.59  0.57\\n    Colour Intensity:              1.3  13.0     5.1   2.3\\n    Hue:                          0.48  1.71    0.96  0.23\\n    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\\n    Proline:                       278  1680     746   315\\n    ============================= ==== ===== ======= =====\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\n\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\n\\nOriginal Owners: \\n\\nForina, M. et al, PARVUS - \\nAn Extendible Package for Data Exploration, Classification and Correlation. \\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\n\\nCitation:\\n\\nLichman, M. (2013). UCI Machine Learning Repository\\n[http://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science. \\n\\nReferences\\n----------\\n(1) \\nS. Aeberhard, D. Coomans and O. de Vel, \\nComparison of Classifiers in High Dimensional Settings, \\nTech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of \\nMathematics and Statistics, James Cook University of North Queensland. \\n(Also submitted to Technometrics). \\n\\nThe data was used with many others for comparing various \\nclassifiers. The classes are separable, though only RDA \\nhas achieved 100% correct classification. \\n(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \\n(All results using the leave-one-out technique) \\n\\n(2) \\nS. Aeberhard, D. Coomans and O. de Vel, \\n\"THE CLASSIFICATION PERFORMANCE OF RDA\" \\nTech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \\nMathematics and Statistics, James Cook University of North Queensland. \\n(Also submitted to Journal of Chemometrics). \\n',\n",
       " 'data': array([[  1.42300000e+01,   1.71000000e+00,   2.43000000e+00, ...,\n",
       "           1.04000000e+00,   3.92000000e+00,   1.06500000e+03],\n",
       "        [  1.32000000e+01,   1.78000000e+00,   2.14000000e+00, ...,\n",
       "           1.05000000e+00,   3.40000000e+00,   1.05000000e+03],\n",
       "        [  1.31600000e+01,   2.36000000e+00,   2.67000000e+00, ...,\n",
       "           1.03000000e+00,   3.17000000e+00,   1.18500000e+03],\n",
       "        ..., \n",
       "        [  1.32700000e+01,   4.28000000e+00,   2.26000000e+00, ...,\n",
       "           5.90000000e-01,   1.56000000e+00,   8.35000000e+02],\n",
       "        [  1.31700000e+01,   2.59000000e+00,   2.37000000e+00, ...,\n",
       "           6.00000000e-01,   1.62000000e+00,   8.40000000e+02],\n",
       "        [  1.41300000e+01,   4.10000000e+00,   2.74000000e+00, ...,\n",
       "           6.10000000e-01,   1.60000000e+00,   5.60000000e+02]]),\n",
       " 'feature_names': ['alcohol',\n",
       "  'malic_acid',\n",
       "  'ash',\n",
       "  'alcalinity_of_ash',\n",
       "  'magnesium',\n",
       "  'total_phenols',\n",
       "  'flavanoids',\n",
       "  'nonflavanoid_phenols',\n",
       "  'proanthocyanins',\n",
       "  'color_intensity',\n",
       "  'hue',\n",
       "  'od280/od315_of_diluted_wines',\n",
       "  'proline'],\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['class_0', 'class_1', 'class_2'], \n",
       "       dtype='|S7')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_loader = load_wine()\n",
    "\n",
    "#find out what this data set is\n",
    "wine_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#will show you if the item is a dictionary or built on a dictionary or not\n",
    "isinstance(wine_loader, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print keys in dictionary\n",
    "\n",
    "for key, val in wine_loader.items():\n",
    "    print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print wine_loader[\"DESCR\"]\n",
    "#will show you the string in an appealing way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <font color=blue>Independent - </font>Create a DataFrame from this strange `wine` object. Don't forget to add column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "#see what is in the wine loader data --> predictor\n",
    "wine_loader[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame[wine_loade.data, columns = wine_loader.feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine_df = pd.DataFrame[wine_loade.data, columns = wine_loader.feature_names]\n",
    "#name it df to tell others that your data is a dataframe\n",
    "\n",
    "y = win_loader.target\n",
    "#will give you the targets / outcomes for your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs for 'pd.DataFrame':\n",
    "    -dictionary\n",
    "     '''python\n",
    "     {\"alcohol\": [14.23, 13.20, ...], 'malic_acid': [..., ..., ...]}\n",
    "     '''\n",
    "     \n",
    "     - a list of lists, where each list is a row\n",
    "     - a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#play around with wine_loader. --> to see what instructions are available. How you find out that certain attributes are feature names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='correlations'></a>\n",
    "## Examine the correlation structure of the dataset\n",
    "\n",
    "---\n",
    "\n",
    "You should exclude the `id` column as this is just an indicator variable for the subject.\n",
    "\n",
    "<a id='heatmap'></a>\n",
    "### Method 1: plot a heatmap of the correlation matrix\n",
    "\n",
    "Plot a seaborn heatmap of the correlation matrix to visually examine which variables are correlated and anti-correlated, and to what degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "\n",
    "wine_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(wine_df.corr())\n",
    "#turns your correlaton into a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pairplot'></a>\n",
    "### Method 2: Use seaborn's pairplot to visualize relationships between variables\n",
    "\n",
    "When you have a small number of predictor variables, seaborn's `pairplot` function will give you a more detailed visual look at the relationships between variables. The pairplot is similar to a correlation matrix, but displays scatterplots of variable pairs. Along the diagonal line are histograms showing the distribution of each variable.\n",
    "\n",
    "One of the most appealing aspects of the pairplot function for classification tasks is that the scatterplots and histograms can be split along a hue variable. If we use the `malignant` target class as the hue we are able to see how the classes are distributed across these variables as well.\n",
    "\n",
    "Plot data using seaborn's `pairplot()` function. The hue will be the class variable \"malignant\". The variables will be the other columns excluding, of course, the subject ID column. This function can take some time to run.\n",
    "\n",
    "> **Note:** Most of these predictors are highly correlated with the \"class\" variable. This is already an indication that our classifier is very likely to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "\n",
    "sns.pairplot.(wine_df.iloc[:, :5])\n",
    "#slice your pair plot data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kneighborsclassifier'></a>\n",
    "\n",
    "## Using sklearn's `KNeighborsClassifier`\n",
    "\n",
    "---\n",
    "\n",
    "Let's see how the sklearn KNN classifier performs on our dataset predicting the malignant target class using cross-validation.\n",
    "\n",
    "Load the KNN classifier like so:\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```\n",
    "\n",
    "**We are going to set some arguments when instantiating the model:**\n",
    "1. **n_neighbors** specifies how many neighbors will vote on the class\n",
    "2. **weights** uniform weights indicate that all neighbors have the same weight\n",
    "3. **metric** and **p**: when distance is minkowski (the default) and p == 2 (the default), _this is equivalent to the euclidean distance metric_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "\n",
    "#1. import\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "#2. instantiate - make one specific instance of KNN by assigning it to a variable, make sure you have the parentheses!\n",
    "\n",
    "knn = KneighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "#3. fit (aka \"train\" on your labeled data)\n",
    "\n",
    "knn.fit(wine_df, y)\n",
    "#first item is the predictor (many items), second item is the target (1 type of data)\n",
    "#by default it uses 5 neighbors\n",
    "\n",
    "#4. Make new predictions or score\n",
    "#randomly select 3 rows and make a prediction on them\n",
    "wine_sample = wine_df.sample(3)\n",
    "print \"prediction:\", knn.predict(wine_df.sample)\n",
    "print \"actual: \", y[wine_sample.index]\n",
    "#will show you what the actual is for the indexes you have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='target-predictors'></a>\n",
    "### Create your target vector and predictor matrix\n",
    "\n",
    "The target should be the binary `malignant` column. The predictors are up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A: see above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit our first K-nearest neighbors model to the wine data\n",
    "\n",
    "The steps to using an sklearn model are:\n",
    "1. Instantiated the model\n",
    "2. Fit the instantiated model object to day (`X` and `y`)\n",
    "3. Score or make predictions with your \"trained model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "### Calculate the \"baseline\" accuracy\n",
    "\n",
    "Before we can evaluate whether our classifier's accuracy is good or bad, we need to know the baseline accuracy.\n",
    "\n",
    "**The baseline accuracy is the proportion of the majority class.**\n",
    "\n",
    "For a binary classification, this means that the baseline accuracy is the percent of the dataset that is labeled malignant or benign, depending on whichever of malignant or benign is greater. This can be calculated:\n",
    "\n",
    "```python\n",
    "baseline = np.mean(y)  # if np.mean(y) is >= 0.5\n",
    "baseline = 1. - np.mean(y) # if np.mean(y) is < 0.5\n",
    "```\n",
    "\n",
    "**It is critical that you know your baseline accuracy!**\n",
    "\n",
    "If your dataset for example had 95 1's and 5 0's, and you got a 95% accuracy using KNN, if you had not looked at your baseline accuracy you may conclude that your classifier is doing great. In fact, it's doing no better than chance! The classifier could have guessed only 1's and gotten a 95% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "\n",
    "#turn y from a numpy to series\n",
    "pd.Series(y).value_counts()\n",
    "#-->see the distribution of your data\n",
    "#baseline accuracy is what is the accuracy if you just predicted the most common class for every row?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts() / len(y)\n",
    "#highest output is the baseline accuracy (most common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn5'></a>\n",
    "### What is the accuracy for a KNN model with 5 neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "\n",
    "knn.score(wine_df, y)\n",
    "#shows what your baseline accuracy is with your training set\n",
    "#need to include the y because then you can compare your predictions to your actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = knn.predict(wine_df)\n",
    "#make a prediction on all y values\n",
    "\n",
    "y_hat == y\n",
    "#will see how well your predictions match the actual y values\n",
    "\n",
    "[val for val in y_hat == y if val == True]\n",
    "#or\n",
    "\n",
    "(y_hat == y).mean()\n",
    "#does the same thinig as above\n",
    "#knn very sensitive to your data - helps if you eliminate bad data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn1'></a>\n",
    "## Tuning your model for performance\n",
    "\n",
    "As you can see the accuracy is already very high with 5 neighbors on the full dataset, but in industry, mere percentage point gains in performance could be a matter of millions of dollars. Let's see how well we can do!\n",
    "\n",
    "Right now we have two main dials to turn:\n",
    "    1. Feature selection ie which column or columns to include in the training set\n",
    "    2. Choice of `n_neighbors` aka `k`\n",
    "\n",
    "### <font color=blue>Partner Work (25 minutes)</font>   - Feature selection\n",
    "We are going to start on feature selection. With your partner, start by exploring which columns will produce the best accuracy score using the default `n_neighbors=5`.\n",
    "\n",
    "After our exploration, we will convene to share our findings as a class.\n",
    "\n",
    "#### Teams\n",
    "\n",
    "<img src=https://i.imgur.com/HFFCeUH.png width=\"30%\" align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Partner Work (25 minutes)</font>   - Hyper-parameter tuning\n",
    "\n",
    "Using the best features, explore what the best value of `k` is for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Feature selection strategies\n",
    "- Segmenting based on the different class labels\n",
    "  - Group bys\n",
    "  - Overlaid histograms\n",
    "- Trial and error\n",
    "- Brute force\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(1,30)[::-1]:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(wine_df, y)\n",
    "    print (k, knn.score(wine_df, y))\n",
    "    \n",
    "#doesn't work with the training set because you're scoring based on yourself and will get higher scores when you have fewer neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Introduction to training and test sets\n",
    " \n",
    "Our model has already seen and fit on the train data that we are using to produce an accuracy score.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*-8_kogvwmL1H6ooN1A1tsQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a test that simulates fresh data that model might be predicting on when it is put into production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tran, X_test, y_train, y_test = train_test_split(wine_df, y, test_size = 0.25)\n",
    "#need to do it on the wine_df and y explicitly to keep order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are our choice of hyper-parameters and features the same as they were when we were validating based on a training set alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train on a set of data and test on your new set of data\n",
    "\n",
    "for k in range(1,30)[::-1]:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print (k, knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Scott Foreman-Roe's [breakdown](http://scott.fortmann-roe.com/docs/BiasVariance.html) (required) of the bias-variance tradeoff featuring a discussion of KNN is an excellent read\n",
    "- A [detailed discussion](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/) of KNN\n",
    "- A long, applied example of KNN applied to [image classification](http://cs231n.github.io/classification/ )\n",
    "- If academic breakdowns are your thing, be sure to visit [this](http://me.seekingqed.com/files/intro_KNN.pdf) resource\n",
    "- Read the SKLearn [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on implementing KNN\n",
    "- Choosing the right [algorithm from SKLearn](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "- A deeper dive into [Euclidian distance](http://www.econ.upf.edu/~michael/stanford/maeb4.pdf)\n",
    "- Classifier comparsion from [SKLearn](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) (this is also in our [repository](https://github.com/ga-students/DSI-DC-2/blob/master/curriculum/Week-04/4.01%20Intro%20to%20Classification/classification-methods.py))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
