{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Classification with K-Nearest Neighbors\n",
    "\n",
    "_Authors: Kiefer Katovich (SF), Alexander Barriga (SF), Joseph Nelson (DC)_\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand the difference between classification and regression models\n",
    "- Understand the K-Nearest Neighbors algorithm visually and in pseudocode\n",
    "- Explain the differences between distance metrics and explore the two most common\n",
    "- Apply KNN classification to the Wisconsin breast cancer dataset\n",
    "- Practice manually performing stratified cross-validation\n",
    "- Visually examine the effect of K neighbors on the decision boundary\n",
    "- Explain the effect of choosing K on the bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Introduction: regression vs. classification\n",
    "\n",
    "We've discussed the difference between continuous and discrete numbers. That difference is what distinguishes classification from regression prediction tasks. Today we are going to focus on predicting non-quantitative, discrete categories, which is known as classification.\n",
    "\n",
    "Take wine for example. You could predict a quality rating using regression, but what if we just wanted to predict whether wine was good or bad? Red or white? \n",
    "\n",
    "Classification algorithms do just that; they predict categories, or classes. Split the data into groups and place new data into those groups. \n",
    "\n",
    "![](http://ipython-books.github.io/images/ml.png \"Best Split vs Best Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn-visual-intro'></a>\n",
    "\n",
    "---\n",
    "\n",
    "### K-Nearest Neighbors (KNN) visually\n",
    "\n",
    "**KNN works similarly to how we humans might choose to classify things. Below we have some red and blue dots:**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_reds_and_blues.png \"Some Dots\")\n",
    "\n",
    "**A new dot appears without a color and we need to decide which color it is most likely going to be.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point.png \"A New Dot Appears\")\n",
    "\n",
    "**We compare it to its three nearest neighbors â€“ its neighbors are more often red, so we label it red.**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred.png \"3 Nearest Neighbors\")\n",
    "\n",
    "**What if we increase the number of neighbors to consider to 5?**\n",
    "![Alt text](http://blog.yhat.com/static/img/knn_new_point_pred_blue.png \"5 Nearest Neighbors\")\n",
    "\n",
    "**This is in essence the K-Nearest Neighbors (KNN) algorithm. The K represents the number of \"neighbors\" you use.**\n",
    "\n",
    "> ***Images above credited to the yhat blog.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='knn'></a>\n",
    "\n",
    "## The KNN algorithm\n",
    "\n",
    "---\n",
    "\n",
    "K-Nearest Neighbors takes a unique approach to finding patterns in the data. In order to estimate a value (regression) or class membership (classification), the algorithm finds the observations in its training data that are \"nearest\" to the observation to predict. It then takes a vote of those training observations' target values to estimate the value for the new data point.\n",
    "\n",
    "Distance is usually calculated using the euclidean distance. The \"K\" in KNN refers to the number of nearest neighbors that will be contributing to the prediction. \n",
    "\n",
    "Today we will be looking at KNN only in the context of classification.\n",
    "\n",
    "**The KNN can be concisely represented with pseudocode:**\n",
    "\n",
    "```\n",
    "for unclassified_point in sample:\n",
    "    for known_point in known_class_points:\n",
    "        calculate distances (euclidean or other) between known_point and unclassified_point\n",
    "    for k in range of specified_neighbors_number:\n",
    "        find k_nearest_points in known_class_points to unclassified_point\n",
    "    assign class to unclassified_point using \"votes\" from k_nearest_points\n",
    "```\n",
    "\n",
    "> **Note**: in the case of ties, sklearn's `KNeighborsClassifier()` will just choose the first class (when weights are uniform)! If this is unappealing to you you can change the weights keyword argument to 'distance'. More on this later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='distance'></a>\n",
    "## The KNN distance metric\n",
    "\n",
    "---\n",
    "KNN typically uses one of two distance metrics: euclidean or manhattan. Other distance metrics are possible, but more rare (sometimes it makes sense to create your own distance function.\n",
    "\n",
    "<a id='euclidean'></a>\n",
    "### Euclidean distance\n",
    "\n",
    "Recall the famous Pythagorean Theorem\n",
    "![Alt text](http://ncalculators.com/images/pythagoras-theorem.gif)\n",
    "\n",
    "We can apply the theorem to calculate distance between points. This is called Euclidean distance. \n",
    "\n",
    "![Alt text](http://rosalind.info/media/Euclidean_distance.png)\n",
    "\n",
    "### $$\\text{Euclidean  distance}=\\sqrt{(x_1-x_2)^2+(y_1-y_1)^2}$$\n",
    "\n",
    "There are many different distance metrics, but Euclidean is the most common (and default in sklearn).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wisconsin'></a>\n",
    "\n",
    "## Load the wine dataset\n",
    "\n",
    "---\n",
    "\n",
    "Below we will be testing out the KNN classification algorithm on the classic [UCI Wine Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### <font color=blue>Independent - </font>Create a DataFrame from this strange `wine` object. Don't forget to add column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='correlations'></a>\n",
    "## Examine the correlation structure of the dataset\n",
    "\n",
    "---\n",
    "\n",
    "You should exclude the `id` column as this is just an indicator variable for the subject.\n",
    "\n",
    "<a id='heatmap'></a>\n",
    "### Method 1: plot a heatmap of the correlation matrix\n",
    "\n",
    "Plot a seaborn heatmap of the correlation matrix to visually examine which variables are correlated and anti-correlated, and to what degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pairplot'></a>\n",
    "### Method 2: Use seaborn's pairplot to visualize relationships between variables\n",
    "\n",
    "When you have a small number of predictor variables, seaborn's `pairplot` function will give you a more detailed visual look at the relationships between variables. The pairplot is similar to a correlation matrix, but displays scatterplots of variable pairs. Along the diagonal line are histograms showing the distribution of each variable.\n",
    "\n",
    "One of the most appealing aspects of the pairplot function for classification tasks is that the scatterplots and histograms can be split along a hue variable. If we use the `malignant` target class as the hue we are able to see how the classes are distributed across these variables as well.\n",
    "\n",
    "Plot data using seaborn's `pairplot()` function. The hue will be the class variable \"malignant\". The variables will be the other columns excluding, of course, the subject ID column. This function can take some time to run.\n",
    "\n",
    "> **Note:** Most of these predictors are highly correlated with the \"class\" variable. This is already an indication that our classifier is very likely to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kneighborsclassifier'></a>\n",
    "\n",
    "## Using sklearn's `KNeighborsClassifier`\n",
    "\n",
    "---\n",
    "\n",
    "Let's see how the sklearn KNN classifier performs on our dataset predicting the malignant target class using cross-validation.\n",
    "\n",
    "Load the KNN classifier like so:\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "```\n",
    "\n",
    "**We are going to set some arguments when instantiating the model:**\n",
    "1. **n_neighbors** specifies how many neighbors will vote on the class\n",
    "2. **weights** uniform weights indicate that all neighbors have the same weight\n",
    "3. **metric** and **p**: when distance is minkowski (the default) and p == 2 (the default), _this is equivalent to the euclidean distance metric_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='target-predictors'></a>\n",
    "### Create your target vector and predictor matrix\n",
    "\n",
    "The target should be the binary `malignant` column. The predictors are up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit our first K-nearest neighbors model to the wine data\n",
    "\n",
    "The steps to using an sklearn model are:\n",
    "1. Instantiated the model\n",
    "2. Fit the instantiated model object to day (`X` and `y`)\n",
    "3. Score or make predictions with your \"trained model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "### Calculate the \"baseline\" accuracy\n",
    "\n",
    "Before we can evaluate whether our classifier's accuracy is good or bad, we need to know the baseline accuracy.\n",
    "\n",
    "**The baseline accuracy is the proportion of the majority class.**\n",
    "\n",
    "For a binary classification, this means that the baseline accuracy is the percent of the dataset that is labeled malignant or benign, depending on whichever of malignant or benign is greater. This can be calculated:\n",
    "\n",
    "```python\n",
    "baseline = np.mean(y)  # if np.mean(y) is >= 0.5\n",
    "baseline = 1. - np.mean(y) # if np.mean(y) is < 0.5\n",
    "```\n",
    "\n",
    "**It is critical that you know your baseline accuracy!**\n",
    "\n",
    "If your dataset for example had 95 1's and 5 0's, and you got a 95% accuracy using KNN, if you had not looked at your baseline accuracy you may conclude that your classifier is doing great. In fact, it's doing no better than chance! The classifier could have guessed only 1's and gotten a 95% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn5'></a>\n",
    "### What is the accuracy for a KNN model with 5 neighbors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cv-knn1'></a>\n",
    "## Tuning your model for performance\n",
    "\n",
    "As you can see the accuracy is already very high with 5 neighbors on the full dataset, but in industry, mere percentage point gains in performance could be a matter of millions of dollars. Let's see how well we can do!\n",
    "\n",
    "Right now we have two main dials to turn:\n",
    "    1. Feature selection ie which column or columns to include in the training set\n",
    "    2. Choice of `n_neighbors` aka `k`\n",
    "\n",
    "### <font color=blue>Partner Work (25 minutes)</font>   - Feature selection\n",
    "We are going to start on feature selection. With your partner, start by exploring which columns will produce the best accuracy score using the default `n_neighbors=5`.\n",
    "\n",
    "After our exploration, we will convene to share our findings as a class.\n",
    "\n",
    "#### Teams\n",
    "\n",
    "<img src=https://i.imgur.com/HFFCeUH.png width=\"30%\" align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=blue>Partner Work (25 minutes)</font>   - Hyper-parameter tuning\n",
    "\n",
    "Using the best features, explore what the best value of `k` is for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Introduction to training and test sets\n",
    " \n",
    "Our model has already seen and fit on the train data that we are using to produce an accuracy score.\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*-8_kogvwmL1H6ooN1A1tsQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a test that simulates fresh data that model might be predicting on when it is put into production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are our choice of hyper-parameters and features the same as they were when we were validating based on a training set alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resources'></a>\n",
    "\n",
    "## Additional resources\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- Scott Foreman-Roe's [breakdown](http://scott.fortmann-roe.com/docs/BiasVariance.html) (required) of the bias-variance tradeoff featuring a discussion of KNN is an excellent read\n",
    "- A [detailed discussion](https://saravananthirumuruganathan.wordpress.com/2010/05/17/a-detailed-introduction-to-k-nearest-neighbor-knn-algorithm/) of KNN\n",
    "- A long, applied example of KNN applied to [image classification](http://cs231n.github.io/classification/ )\n",
    "- If academic breakdowns are your thing, be sure to visit [this](http://me.seekingqed.com/files/intro_KNN.pdf) resource\n",
    "- Read the SKLearn [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) on implementing KNN\n",
    "- Choosing the right [algorithm from SKLearn](http://scikit-learn.org/stable/tutorial/machine_learning_map/)\n",
    "- A deeper dive into [Euclidian distance](http://www.econ.upf.edu/~michael/stanford/maeb4.pdf)\n",
    "- Classifier comparsion from [SKLearn](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) (this is also in our [repository](https://github.com/ga-students/DSI-DC-2/blob/master/curriculum/Week-04/4.01%20Intro%20to%20Classification/classification-methods.py))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ds42)",
   "language": "python",
   "name": "ds42"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
