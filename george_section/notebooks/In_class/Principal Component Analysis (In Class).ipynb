{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning with Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goals:** \n",
    "\n",
    "- What is PCA (Principal Component Analysis)? How it works and how to use it.\n",
    "- Visualize and interpret PCA-transformed data\n",
    "- Work with PCA-transformed data for classification and clustering purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](images/pca_image.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![we](images/pca_image_transformed.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is the difference between the two images???**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The first image is the high-quality version and the second is the low quality. The first is 16MB and the second 1.2MB\n",
    "\n",
    "<br>\n",
    "\n",
    "Given that information, is it fair to say that these two images are effectively the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA (Principal Component Analysis)\n",
    "--- \n",
    "\n",
    "* Dimension Reductionality technique that when faced with a large set of correlated variables, allow us to summarize this set with a smaller number of representative variables that **collectively** explain most of the variability in the original set.\n",
    "* The quintessential \"dimensionality reduction\" algorithm, where _\"dimensionality reduction\"_ = process of combining or collapsing your existing features (columns in $X$) into new features that retain the signal in the original data in fewer variables while ideally reducing noise.\n",
    "* **PCA** is an unsupervised approach, since it involves only a set of features $X_1$, $X_2$, . . . , $X_p$, and no associated response $Y$ \n",
    "* **PCA** produces derived variables to use in supervised methods and is also a tool for data visualization. Imagine being able to visualize a 100-dimension dataset on a 2D scatter plot.\n",
    "\n",
    "- Like the two images above the PCA can transform data with 100 dimensions into 2 while retaining the \"essence\" of the original data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Sebastian Raschka](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#introduction):\n",
    "\n",
    "\"The sheer size of data in the modern age is not only a challenge for computer hardware but also a main bottleneck for the performance of many machine learning algorithms. The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense. In a nutshell, this is what PCA is all about: Finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The idea is that each of the $n$ observations lives in $p$-dimensional space, but not all of these dimensions are equally interesting.\n",
    "* Each of the dimensions found by **PCA** is a linear combination of the $p$ features, so the first principal component of a set of features $X_1$, $X_2$, . . . , $X_p$ is the **normalized** linear combination of the features:\n",
    "$$Z_1 = œÜ_{11}X_1 +œÜ_{21}X_2 +...+œÜ_{p1}X_p$$\n",
    "that have the **largest variance**  \n",
    "The elements $œÜ_{11}$,...,$œÜ_{p1}$ are the loadings of the **first** PCA, **AND** together these loadings make up the principal component loading vector whose **SUM** is equal to **1**: $$œÜ_1 = (œÜ_{11}  œÜ_{21} ... œÜ_{p1})^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process\n",
    "---\n",
    "* Linearly transform an $ùëÅ$√ó$ùëë$ matrix $ùëã$ into an $ùëÅ$√ó$ùëö$ matrix $ùëå$\n",
    "    * Centralized the data (subtract the mean). \n",
    "    * Calculate the $ùëë$√ó$ùëë$ covariance matrix: $$ùê∂ = \\frac1{N-1} X^T X$$\n",
    "        * $C_{ij}$ =  $\\frac1{N-1}$ $\\sum_{q=1}^N$ $X_{q,i}$ $X_{q,i}$ \n",
    "        * $C_{i,i}$ (diagonal) is the variance of variable $i$\n",
    "        * $C_{i,j}$ (off-diagonal) is the covariance between variables $i$ and $j$\n",
    "    * Calculate the **eigenvectors** of the covariance matrix\n",
    "         * An **eigenvector** specifies a direction through the original coordinate space. \n",
    "    * Select $m$ **eigenvectors** that correspond to the **largest $m$ eigenvalues** to be the new basis.\n",
    "         * The eigenvector with the highest correspoding **eigenvalue** is the first principal component.\n",
    "         * **Eigenvalues** indicate the amount of variance in the direction of it's corresponding eigenvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors\n",
    "---\n",
    "* If $A$ is a **square matrix**, a non-zero vector **$v$** is an **eigenvector** of $A$ if there is a scalar $Œª$ **(eigenvalue)** such that $$Av = Œªv$$\n",
    "\n",
    "* For example:\n",
    "$$ Av = \n",
    "\\left(\\begin{array}{cc} \n",
    "2 & 3\\\\\n",
    "2 & 1\n",
    "\\end{array}\\right) *\n",
    "\\left(\\begin{array}{cc} \n",
    "3 \\\\ \n",
    "2 \n",
    "\\end{array}\\right) = \n",
    "\\left(\\begin{array}{cc} \n",
    "12 \\\\\n",
    "8\n",
    "\\end{array}\\right) = \n",
    "4\n",
    "\\left(\\begin{array}{cc} \n",
    "3 \\\\\n",
    "2\n",
    "\\end{array}\\right)\n",
    "= Œªv\n",
    "$$ \n",
    "$~$\n",
    "* If you think of the squared matrix $A$ as a transformation matrix, then multiply it with the **eigenvector do not change its direction.**\n",
    "\n",
    "<a href = http://setosa.io/ev/eigenvectors-and-eigenvalues/> Please see Eigenvectors and Eigenvalues Visually </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the principal components\n",
    "---\n",
    "\n",
    "![](https://snag.gy/ECsJye.jpg)\n",
    "*Image from Introduction to Statistical Learning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "---\n",
    "\n",
    "* What is a principal component? **Principal components are the vectors that define the new coordinate system for your data.** Transforming your original data columns onto the principal component axes constructs new variables that are optimized to explain as much variance as possible and to be independent (uncorrelated).\n",
    "\n",
    "* Creating these variables is a well-defined mathematical process, but in essence **each component is created as a weighted sum of your original columns, such that all components are orthogonal (perpendicular) to each other**.\n",
    "\n",
    "* Eigenvectors: The direction of the components\n",
    "\n",
    "* Eigenvalues: The lenght of the Eigenvectors, every eigenvector has an Eigenvalue. The magnitude of the Eigenvector encodes the proportion of total variance explained by a component. The whole variance is the same as the number of variables in the PCA. A value of 1 mean it explains as much variance as one variable. >1 means a component explains more variance than a single variable.\n",
    "\n",
    "* We can reduce the number of dimensions (remove bottom number of components) and lose the least possible amount of variance information in our data.\n",
    "* Since we are assuming our variables are interrelated (at least in the sense that they together explain a dependent variable), the information of interest should exist along directions with largest variance.\n",
    "* The directions of largest variance should have the highest Signal to Noise ratio.\n",
    "* Correlated predictor variables (also referred to as \"redundancy\" of information) are combined into independent variables. Our predictors from PCA are guaranteed to be independent.\n",
    "---\n",
    "![](https://snag.gy/0Hur9o.jpg)\n",
    "*Image from http://setosa.io/ev/principal-component-analysis/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Lesson Adapted from Sebastian Raschka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do a walkthrough on this lesson adapted from [Sebastian Raschka](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#introduction) on how to use PCA with the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data and assign X and y\n",
    "\n",
    "iris = \n",
    "X = \n",
    "y = iris.species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize the data\n",
    "\n",
    "scaler = \n",
    "\n",
    "Xs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Eigendecomposition - Computing Eigenvectors and Eigenvalues\n",
    "\n",
    "\"The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the ‚Äúcore‚Äù of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Covariance Matrix**\n",
    "\"The classic approach to PCA is to perform the eigendecomposition on the covariance matrix Œ£, which is a d√ód matrix where each element represents the covariance between two features. The covariance between two features is calculated as follows:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\sigma_{jk} = \\frac{1}{n-1}\\sum_{i=1}^{N}\\left(  x_{ij}-\\bar{x}_j \\right)  \\left( x_{ik}-\\bar{x}_k \\right).\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize the calculation of the covariance matrix via the following matrix equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\Sigma = \\frac{1}{n-1} \\left( (\\mathbf{X} - \\mathbf{\\bar{x}})^T\\;(\\mathbf{X} - \\mathbf{\\bar{x}}) \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where x¬Ø is the mean vector \\begin{align}\\mathbf{\\bar{x}} = \\sum\\limits_{k=1}^n x_{i}.\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean vector is a d-dimensional vector where each value in this vector represents the sample mean of a feature column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use numpy \n",
    "\n",
    "mean_vec = np.mean(Xs, axis = 0)\n",
    "\n",
    "#The mean value of each column\n",
    "mean_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manually Calculate the covariance matrix\n",
    "cov_mat = (Xs - mean_vec).T.dot((Xs - mean_vec))/ (Xs.shape[0] - 1)\n",
    "\n",
    "cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use numpy\n",
    "cov_mat = np.cov(Xs.T)\n",
    "cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive eigenvalues and eigenvectors\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "eig_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correlation Matrix**\n",
    "\n",
    "\"Especially, in the field of \"Finance,\" the correlation matrix typically used instead of the covariance matrix. However, the eigendecomposition of the covariance matrix (if the input data was standardized) yields the same results as a eigendecomposition on the correlation matrix, since the correlation matrix can be understood as the normalized covariance matrix. Eigendecomposition of the standardized data based on the correlation matrix:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_mat1 = np.corrcoef(Xs.T)\n",
    "cor_mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(cor_mat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eigenvectors\n",
    "eig_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Selecting Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The typical goal of a PCA is to reduce the dimensionality of the original feature space by projecting it onto a smaller subspace, where the eigenvectors will form the axes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\n",
    "In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top `k` eigenvectors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "eig_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "print('Eigenvalues in descending order:')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sorting the eigenpairs, the next question is \"how many principal components are we going to choose for our new feature subspace?\" A useful measure is the so-called \"explained variance,\" which can be calculated from the eigenvalues. The explained variance tells us how much information (variance) can be attributed to each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "cum_var_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting variance explained and cumulative variance explained\n",
    "\n",
    "pc_list =[\"PC1\", \"PC2\", \"PC3\", \"PC4\"]\n",
    "plt.figure(figsize=(9, 7))\n",
    "plt.bar([1, 2, 3, 4], var_exp, tick_label= pc_list)\n",
    "plt.plot([1, 2, 3,4], cum_var_exp, c = \"r\")\n",
    "plt.title(\"Explained Variance by Different Principal Components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above clearly shows that most of the variance (72.77% of the variance to be precise) can be explained by the first principal component alone. The second principal component still bears some information (23.03%) while the third and fourth principal components can safely be dropped without losing to much information. Together, the first two principal components contain 95.8% of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's about time to get to the really interesting part: The construction of the projection matrix that will be used to transform the Iris data onto the new feature subspace. Although, the name \"projection matrix\" has a nice ring to it, it is basically just a matrix of our concatenated top k eigenvectors.\n",
    "\n",
    "Here, we are reducing the 4-dimensional feature space to a 2-dimensional feature subspace, by choosing the \"top 2\" eigenvectors with the highest eigenvalues to construct our `d√ók`-dimensional eigenvector matrix `W`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1), \n",
    "                      eig_pairs[1][1].reshape(4,1)))\n",
    "\n",
    "matrix_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Projection Onto the New Feature Space\n",
    "\n",
    "In this last step we will use the 4√ó2-dimensional projection matrix `W` to transform our samples onto the new subspace via the equation\n",
    "`Y=X√óW`, where `Y` is a 150√ó2 matrix of our transformed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the dot product of Xs and matrix_w\n",
    "Y = Xs.dot(matrix_w)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot components with color-encoding\n",
    "\n",
    "colors = y.map({\"setosa\": \"r\", \"virginica\": \"g\", \"versicolor\": \"b\"})\n",
    "plt.figure(figsize=())\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c  = colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA in Sklearn with Iris, the NBA, and Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to learn how to use PCA with the Iris, NBA 16/17 stats, and MNIST digits datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris\n",
    "\n",
    "Let's use the pairplot of iris to compare what that looks like to the two components pca scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pairplot of iris data with species color-encoding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know, we can't visualize a four-dimensional dataset, the pairplot gives use the next best thing, which is visualizing the four-dimensional dataset from as many different perspectives as possible.\n",
    "\n",
    "Now let's compare this pairplot with a scatter plot of the iris' data two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize pca object with 2 components\n",
    "\n",
    "pca = \n",
    "\n",
    "#Fit and transform scaled iris data using pca\n",
    "iris_pca = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the components_ are the eigenvectors. For now, just know that we are using \n",
    "# them to transform the data.\n",
    "first_pc = \n",
    "\n",
    "second_pc = \n",
    "\n",
    "first_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The points presented above indicates the direction of each PCA. lets plot them to see the direction of each \n",
    "\n",
    "x1 = \n",
    "x2 = \n",
    "\n",
    "y1 = \n",
    "y2 = \n",
    "\n",
    "plt.axis([-.5, 2, -0.9, 2])\n",
    "plt.arrow(0,0,x1,x2, color = 'black', width = .07)\n",
    "plt.arrow(0,0,y1,y2, color = 'brown', width = .07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The black arrow shows the direction of the first principal component. the graph above shows the projections of the data into those axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "plt.scatter(iris_pca[:, 0], iris_pca[:, 1], c = colors, alpha=.4, s = 50)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.arrow(0,0,x1,x2, color = 'brown', width = .1)\n",
    "plt.arrow(0,0,y1,y2, color = 'black', width = .1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NBA Player data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the wine data\n",
    "\n",
    "\n",
    "nba = pd.read_csv('../../data/nba_player_data_1617.csv', index_col=[0])\n",
    "\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter dataset\n",
    "\n",
    "cols = [\"PTS\", \"OREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TOV\", \"MIN\", \"EFF\"]\n",
    "\n",
    "nba = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "\n",
    "new_cols = [\"points\", \"offensive_rebs\", \"total_rebounds\", \"assists\", \"steals\", \"blocks\",\n",
    "              \"turnovers\", \"minutes\", \"efficiency\"]\n",
    "\n",
    "nba.columns = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's examine the correlations between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation heatmap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you make of this plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of PCA is that it can deal with multicollinearity pretty well because multicollinearity simply means that you have excess dimensions in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and transform our data with PCA. But first we have to standardize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize scaler\n",
    "\n",
    "scaler =\n",
    "\n",
    "#Fit and transform wine data using standard scaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize PCA object\n",
    "#We're deliberating leaving the n_components parameters alone\n",
    "\n",
    "\n",
    "#Fit and transform wine_s use pca\n",
    "\n",
    "nba_pca = \n",
    "\n",
    "#Number of components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not reduce any of the dimensions of the dataset because we are going to visualize how much variance is explained by all 9 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the variance explained ratio of the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the percentage of the variance explained by each component\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to plot a bar plot of the explained variance ratios and a line plot of the cumulative sum of the explained variance ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "components = range(1, pca.n_components_ + 1)\n",
    "\n",
    "plt.bar(components, pca.explained_variance_ratio_, label = \"Explained Variance Ratio\")\n",
    "plt.plot(components, np.cumsum(pca.explained_variance_ratio_), c = \"r\", label = \"Cumulative Sum of Explained Variance ratios\")\n",
    "plt.xlabel(\"Components\")\n",
    "plt.ylabel(\"Explained Varianced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tell us that if we view this 9-dimension data on a 2D scatter plot then we would be seeing four-fifths of the total variance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = [u'Russell Westbrook', u'James Harden', u'Isaiah Thomas',\n",
    "       u'Anthony Davis', u'DeMar DeRozan', u'Damian Lillard',\n",
    "       u'DeMarcus Cousins', u'LeBron James', u'Kawhi Leonard',\n",
    "       u'Stephen Curry', u'Kyrie Irving', u'Karl-Anthony Towns',\n",
    "       u'Kevin Durant', u'Paul George', u'Andrew Wiggins',\n",
    "        u'John Wall', 'Giannis Antetokounmpo', 'Harrison Barnes',\n",
    "          'Kevin Love', 'DeAndre Jordan', 'Serge Ibaka', 'Enes Kanter', 'Rudy Gobert', u'Al Horford',\n",
    "          'Dwight Howard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba_pca_df = pd.DataFrame(nba_pca[:, :2], columns=[\"pc1\", \"pc2\"], index=nba.index)\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.scatter(nba_pca_df.pc1, nba_pca_df.pc2)\n",
    "\n",
    "for player in players:\n",
    "    data = nba_pca_df.loc[player]\n",
    "    plt.annotate(player, (data[\"pc1\"], data[\"pc2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you label the components based on the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the component weights with their corresponding variables for PC1, PC2, and PC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Component 1\n",
    "for col, comp in zip(nba.columns, pca.components_[0]):\n",
    "    print col, comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Component 2\n",
    "for col, comp in zip(nba.columns, pca.components_[1]):\n",
    "    print col, comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Component 3\n",
    "for col, comp in zip(nba.columns, pca.components_[2]):\n",
    "    print col, comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the mnist digits data\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "data = digits.data\n",
    "numbers = digits.target\n",
    "\n",
    "#shape of data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's PCA the digits data using two components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize PCA with 2 components\n",
    "pca = PCA(2)  # project from 64 to 2 dimensions\n",
    "\n",
    "data_pca = pca.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the prinicpal components of the digits dataset using the number labels as the color-encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(data_pca[:, 0], data_pca[:, 1],\n",
    "            c=numbers, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('spectral', 10))\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explained variance of the pca data\n",
    "\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 components gets us about 28.5 of the way there, let's see how many components it takes to get 50, 70, and 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can set n_components equal to a explained variance ratio value\n",
    "\n",
    "#50% EVR\n",
    "pca = PCA(n_components=.5).fit(data)\n",
    "\n",
    "#Number components\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#70% EVR\n",
    "pca = PCA(n_components=.7).fit(data)\n",
    "\n",
    "#Number components\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#90% EVR\n",
    "pca = PCA(n_components=.9).fit(data)\n",
    "\n",
    "#Number components\n",
    "pca.n_components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize EVR curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize and fit digits data using PCA\n",
    "pca = PCA().fit(data)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "- https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "- http://setosa.io/ev/principal-component-analysis/\n",
    "- https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/\n",
    "- https://github.com/viniciuspantoja/tutorial_PCA/blob/master/Tutorial_PCA.ipynb\n",
    "- http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html\n",
    "- https://towardsdatascience.com/reducing-dimensionality-from-dimensionality-reduction-techniques-f658aec24dfe\n",
    "- https://towardsdatascience.com/exploratory-data-analysis-into-the-relationship-between-different-types-of-crime-in-london-20c328e193ff\n",
    "- https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In class\n",
    "\n",
    "1. Use PCA on a supervised learning dataset we've worked with previously to see if PCA-transformed data can produce models as good as the original data.\n",
    "\n",
    "2. Use time to work on final project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
