{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression, Regularization with Lasso and Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Goals</b>\n",
    "\n",
    "- Polynomial, Lasso, and Ridge regression. How they work and how to use them.\n",
    "- The concept of Regularization. Why it's used and how it relates to the external bias vs variance problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "So far we've only tried to create regression models that are linear functions of the predictor variables.  However, there's no reason we can't transform the predictor variables by any type of function we want before inputting them to linear regression.  This is the idea behind [**Polynomial Regression**](https://en.wikipedia.org/wiki/Polynomial_regression) and it allows us (along with similar functional regressions) to essentially model our response variables as any function of our predictor variables that we like.  Viewed in this way, Linear Regression is just a special instance of Polynomial Regression with a polynomial of degree 1.\n",
    "\n",
    "We're going to use polynomial terms (squares, cubes, etc..) on a regression equation for the purpose of fitting non-linear data, which is data cannot be fit using a least squares model.\n",
    "\n",
    "![formula](http://www.statisticshowto.com/wp-content/uploads/2015/01/excel-polynomial-regression.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Polynomial Regression with sklearn\n",
    "sklearn has built-in options for converting your predictor variables to polynomial functions of them.  In this exercise we'll use the [**PolynomialFeatures**](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class of sklearn to manipulate incoming predictors into nth-order polynomials of those features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make random data to plot\n",
    "#Input variable of 300 numbers from 0 to 30\n",
    "X = np.linspace(0, 30, 300)\n",
    "#Square x and assign it to y\n",
    "y = (X**2)\n",
    "#Throw in some randomness into y\n",
    "y = y*(np.random.rand(300)*5+ 2)\n",
    "#plt x and y\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is clearly non linear but let's go ahead fit a linear model on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape X\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "#Intialize linear regression model\n",
    "\n",
    "#Fit model\n",
    "\n",
    "\n",
    "#Make predictions\n",
    "\n",
    "\n",
    "#Print score\n",
    "print \n",
    "#plot data and predictions\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X, y, alpha=.6)\n",
    "plt.plot(X, preds, \"r\", linewidth = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is the fit?\n",
    "\n",
    "Let's use polynomial regression to model this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Polynomial Features object with degree = 2\n",
    "\n",
    "\n",
    "\n",
    "#fit and transform Polynomial object on data, set equal to X2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X2 shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at first column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at second column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at third column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is an intercept, the second is the original, and the third is the transformed column.\n",
    "\n",
    "Now let's throw this into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize model\n",
    "\n",
    "\n",
    "#fit model on X2 \n",
    "\n",
    "\n",
    "#Make predictions\n",
    "\n",
    "\n",
    "#Score model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data and predictions\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "#Plot the original X data versus\n",
    "\n",
    "#Plot line plot of X and the new predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, here's our polynomial regression plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Set random seeed\n",
    "np.random.seed(9)\n",
    "\n",
    "# Function that returns the sin(2*pi*x)\n",
    "def f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "# This returns 100 evenly spaced numbers from 0 to 1\n",
    "X = \n",
    "\n",
    "n_samples = 100\n",
    "\n",
    "#Pass X into f and add random data to jitter data\n",
    "y = \n",
    "\n",
    "#Make \"predictions\" set equal to curve\n",
    "curve = \n",
    "\n",
    "# Plot the training data against what we know to be the ground truth sin function\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting an nth-degree Polynomial\n",
    "Now that we have our data and know the ground truth, let's try fitting a 3rd degree polynomial to our training data and see how it looks.  3rd degree makes sense for this interval because the sin function has 2 turning points over the interval [0,1] and a 3rd degree polynomial will general have 2 (or less) turning points.\n",
    "\n",
    "We'll combine the PolynomialFeatures function with the [make_pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) function to string together a pipeline of operations that is able to first transform our linear features into polynomial features and then run a linear regression against the resulting polynomial features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Assign a variable pip to make_pipeline function that tkes in Polynom and Linreg objects\n",
    "#Specify degree = 3\n",
    "\n",
    "\n",
    "#Reshape X\n",
    "Xrs =\n",
    "#fit pipe object on X and y\n",
    "\n",
    "#Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set size\n",
    "plt.figure(figsize=(9, 6))\n",
    "#Plot curve line\n",
    "\n",
    "#Scatter plot of data\n",
    "\n",
    "#Plot predictions from pipe\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you make of the fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code in function form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_ploting(X, y, degree):\n",
    "    pipe = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())\n",
    "    #Reshape X\n",
    "    Xrs = X.reshape(-1, 1)\n",
    "    #fit pipe object on X and y\n",
    "    pipe.fit(Xrs, y)\n",
    "    preds = pipe.predict(Xrs)\n",
    "    print (\"R2 score is\", pipe.score(Xrs, y))\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    #Plot curve line\n",
    "    plt.plot(X, curve, label='Curve Line', linewidth = 3, color='green')\n",
    "    #Scatter plot of data\n",
    "    plt.scatter(X, y, label='data', s=100)\n",
    "    #Plot predictions from pipe\n",
    "    plt.plot(X, preds,label= \"Predicted Line with {} degrees\".format(degree),\n",
    "             linewidth = 3, color= \"red\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot with 2 degrees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot with 5 degrees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give me a value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give me another one but higher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One last one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice happens when we increase our degrees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the polynomial degree increase so does the overfitting of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![overfit](reg_overfit.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "In simple terms it is the penalty on a model's complexity. Regularization helps prevent overfitting by penalizing large coefficients.\n",
    "\n",
    "- If your model is very complex (i.e., lots of features, large features, high degree polynomial fit, etc.), you need to worry more about overfitting.\n",
    "- Increasing model complexity leads multicolinearity because OLS-derived coefficients become increasingly large. This is a sign that the model is incorporating too much variance from the dataset aka <b>Overfitting</b>\n",
    "- The example below uses the same dataset as above, but with fewer samples, and a relatively high degree model.\n",
    "- Ridge and Lasso regression are two different regularization models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](https://image.slidesharecdn.com/ch-4-demand-estimation2-110225045402-phpapp01/95/ch-4demandestimation2-11-728.jpg?cb=1298609672)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Ridge</b>: It's function is the standard OLS function plus the squared value of each coefficient multipled by a constant/parameter (alpha) determined by you the data scientist. Similar to picking a K in a KNN model. 0 alpha equals OLS, very high alpha equals underfitting. Aka L2 regularization.\n",
    "![ridge](ridge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha or $\\lambda$ can take any value greater than 0. Best way to choose an optimal regularization parameter is with cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lasso</b>: OLS function plus the sum of the absolute values of each coefficient. Has an advantage over Ridge because it's great for feature selection because it shrinks values of insignificant features to 0. Aka L1 regularization.\n",
    "![e](lasso.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge is better for dealing with multicollinearity and Lasso is better for high number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in boston dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "#Turn into pandas dataframe\n",
    "df = pd.DataFrame(boston[\"data\"])\n",
    "df.columns = boston[\"feature_names\"]\n",
    "df[\"MEDV\"] = boston[\"target\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the data dictionary\n",
    "boston[\"DESCR\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "X = df.drop(\"MEDV\", axis =1)\n",
    "y = df.MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and score a linear regression model\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, y)\n",
    "lr.score(X ,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and score a ridge regression model with alpha = 0.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit and score a lasso regression model with alpha = 0.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we make of these results? How do the regularized models compare to the linear one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the best features using lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the lasso coefficients to coef variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zip column names and coef\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso assigns a 0 to features it thinks are useless, so what matters is the magnitude and not the direction of the feature coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to cross-validate with the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use cross_val_score function on linear regression and set scoring = r2\n",
    "cross_val_score(LinearRegression(), X, y, cv = 5, scoring = \"r2\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use cross_val_score function on ridge regression with alpha = 100 and set scoring = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use cross_val_score function on lasso regression with alpha = 1 and set scoring = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the cross-validated scores of the Lasso/Ridge regressions compare to that of Linear regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this exercise again but with the polynomial transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the features to 2-degrees\n",
    "poly = PolynomialFeatures(2)\n",
    "Xp = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit linear regression model on the Xp features and evaluate it on itself\n",
    "lr = LinearRegression()\n",
    "lr.fit(Xp, y)\n",
    "lr.score(Xp, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Ridge regression model with 1500000 alpha on the Xp features and evaluate it on itself\n",
    "rid = Ridge(1500000)\n",
    "rid.fit(Xp, y)\n",
    "rid.score(Xp, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Lasso regression model with 50 alpha on the Xp features and evaluate it on itself\n",
    "las = Lasso(50)\n",
    "las.fit(Xp, y)\n",
    "las.score(Xp, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the Ridge & Lasso scores compare to that of the Linear score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression cross-validation\n",
    "cross_val_score(LinearRegression(), Xp, y, cv = 5, scoring = \"r2\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does a negative r-squared score mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge regression cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso regression cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We've cross-validated six different data and algorithm combinations. What have we learned from this part?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class exercise:** \n",
    "\n",
    "\n",
    "- We're going to find the best alpha values for both ridge and lasso models trained on polynomial-transformed features.\n",
    "\n",
    "- I want you plot a range of alpha values versus the cross validated r-squared scores aka a validation curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined ridge_alphas\n",
    "ridge_alphas = np.logspace(3, 7, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alpha values for lasso model\n",
    "lasso_alphas = np.linspace(25, 160, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Polynomial Regression cont."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to continue our lesson regularized polynomial regression by modeling higher degree data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create random data and pass it into function\n",
    "func = lambda x: 1 + .1 * (x - 4) ** 2 + 4 * np.random.random(len(x))\n",
    "N, n = 1000, 30\n",
    "domain = np.linspace(0, 15, N)\n",
    "x_sample = np.linspace(0, 15, n)\n",
    "y_sample = func(x_sample)\n",
    "#Plot random data\n",
    "plt.scatter(x_sample, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to reshape data\n",
    "X = np.array([x_sample]).T\n",
    "for degree in [1, 3, 8, 13]:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression()).fit(X, y_sample)\n",
    "    y_pred = model.predict(np.array([domain]).T)\n",
    "    plt.plot(domain, y_pred, alpha=.5, label=\"deg %d (R2 %.2f)\" % (degree, model.score(X, y_sample)))\n",
    "\n",
    "plt.scatter(x_sample, func(x_sample))\n",
    "plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_sample, func(x_sample), label=\"samples\")\n",
    "for degree in [1, 2, 3, 4, 5]:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    # Compute a few R2 scores and print average performance\n",
    "    scores = []\n",
    "    for k in range(5):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_sample, train_size=.7)\n",
    "        scores.append(model.fit(X_train, y_train).score(X_test, y_test))\n",
    "    print (\"For degree\", degree, \", R2 =\", np.mean(scores))\n",
    "    # Take last model to plot predictions\n",
    "    y_pred = model.predict(np.array([domain]).T)\n",
    "    plt.plot(domain, y_pred, alpha=.5, label=\"deg %d (R2 %.2f)\" % (degree, model.score(X_test, y_test)))\n",
    "\n",
    "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for cross validating various polynomial values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(test_model, X, y_sample):\n",
    "    scores = {'overfit': {}, 'cv': {}}\n",
    "    for degree in range(1, 20):\n",
    "        model = make_pipeline(PolynomialFeatures(degree), test_model)    \n",
    "        scores['overfit'][degree] = model.fit(X, y_sample).score(X, y_sample)\n",
    "        cv_scores = []\n",
    "        for k in range(5):  # Compute a few R2 scores and print average performance\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y_sample, train_size=.7)\n",
    "            cv_scores.append(model.fit(X_train, y_train).score(X_test, y_test))\n",
    "        scores['cv'][degree] = np.mean(cv_scores)\n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass in empty linear regression and X and y_sample\n",
    "scores = analyze_performance(LinearRegression(), X, y_sample)\n",
    "scores.plot(ylim=(-.05,1.05))\n",
    "plt.title(\"Best cv performance at degree %d\" % scores.cv.argmax()), plt.xlabel('degree'), plt.ylabel('$R^2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run this code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_small_sample = x_sample[::4]\n",
    "y_small_sample = func(x_small_sample)\n",
    "\n",
    "degree, alpha = 4, 10\n",
    "\n",
    "X = np.array([x_small_sample]).T\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 4))\n",
    "m = [\"Linear\", \"Ridge\", \"Lasso\"]\n",
    "for no, my_model in enumerate([LinearRegression(), Ridge(alpha=alpha), Lasso(alpha=alpha)]):    \n",
    "    model = make_pipeline(PolynomialFeatures(degree), my_model)    \n",
    "    r2, MSE = [], []\n",
    "    for k in range(40):  # Fit a few times the model to different training sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_small_sample, train_size=.7)\n",
    "        r2.append(model.fit(X_train, y_train).score(X_test, y_test))\n",
    "        y_pred = model.predict(np.array([domain]).T)\n",
    "        axes[no].plot(domain, y_pred, alpha=.3)\n",
    "        y_pred_sample = model.predict(np.array([x_small_sample]).T)\n",
    "        MSE.append(np.square(y_pred_sample - y_small_sample).sum())\n",
    "    axes[no].scatter(x_small_sample, y_small_sample, s=70)\n",
    "    axes[no].set_title(\"%s  MSE %3d)\" % (m[no], np.mean(MSE)))\n",
    "    axes[no].set_xlim(-.2, max(domain)), axes[no].set_ylim(-1, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](lin_ridge_lasso.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The unregularized `LinearRegression` leads to a model that is too complex and tries to fit the noise. \n",
    "- Note the differences in the (averaged) mean square error, or MSE, as well the complexity in the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's incorpoate polynomial degrees a few degrees with the regularized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = [LinearRegression(), Ridge(alpha=10), Lasso(alpha=10)]\n",
    "\n",
    "scores = [analyze_performance(my_model, X, y_sample) for my_model in test_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(20, 4))\n",
    "for no, score in enumerate(scores):\n",
    "    s, name = pd.DataFrame(score), test_models[no].__class__.__name__\n",
    "    f = s.plot(ylim=(-.05,1.05), ax=axes[no], legend=False)\n",
    "    f = axes[no].set_title(\"%s\\nBest cv performance at degree %d\" % (name, s.cv.argmax()))\n",
    "    f = axes[no].set_xlabel('degree'), axes[no].set_ylabel('$R^2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a few different values for $\\alpha$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(18, 6))\n",
    "test_models = test_models[1:]\n",
    "for col, alpha in enumerate([0, 1, 10, 100]):\n",
    "    scores = [analyze_performance(my_model, X, y_sample) for my_model in test_models]\n",
    "    for row, score in enumerate(scores):\n",
    "        s, name = pd.DataFrame(score), test_models[row].__class__.__name__\n",
    "        f = s.plot(ylim=(-.05,1.05), ax=axes[row, col], legend=False)\n",
    "        f = axes[row, col].set_title(\"%s (alpha %d)\\nBest cv at degree %d\" % (name, alpha, s.cv.argmax()))\n",
    "        f = axes[row, col].set_xlabel('degree'), axes[row, col].set_ylabel('$R^2$')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Resources:\n",
    "\n",
    "- https://www.theanalysisfactor.com/regression-modelshow-do-you-know-you-need-a-polynomial/\n",
    "- http://connor-johnson.com/2014/02/18/linear-regression-with-python/\n",
    "- http://blog.minitab.com/blog/adventures-in-statistics-2/curve-fitting-with-linear-and-nonlinear-regression\n",
    "- https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n",
    "- https://www.youtube.com/watch?v=nmHNXsDPPFQ\n",
    "- https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html\n",
    "- https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/\n",
    "- http://setosa.io/ev/ordinary-least-squares-regression/\n",
    "- https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\n",
    "- http://www.few.vu.nl/~wvanwie/Courses/HighdimensionalDataAnalysis/WNvanWieringen_HDDA_Lecture23_RidgeRegression_20172018.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class work\n",
    "\n",
    "\n",
    "For the rest of class, work on modeling the King County housing dataset using the models and techniques we've covered in this class. Find out if lasso and ridge regressions can give us a better model with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc = pd.read_csv(\"../data/kc_house_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
