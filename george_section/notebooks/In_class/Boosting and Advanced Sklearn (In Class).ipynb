{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting, Classification Metrics and Advanced Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Goals</b>\n",
    "\n",
    "- Follow up our lesson on ensemble methods with boosting, what it is and how it works.\n",
    "- Use the Adaptive (ADA) Boosting Classifier.\n",
    "- Refresher lesson on model evaluation tolls beyond accuracy score: sensitivity, recall, precision, and roc_auc\n",
    "- How to use high-powered tools in sklearn to optimize your models and minimize your work load and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Boosting is an ensemble method where a model is comprised of a sequence of models, as opposed to a set of parallel models as with Random Forest.\n",
    "- Unlike bagging, boosting uses random subsets of training data <b>WITHOUT</b> replacement.\n",
    "- It is an iterative process. Begins by training simple model on the whole data, pinpoints the inaccuracies, and trains a new model to target those inaccuracies (misclassification rate, residuals.) The new models try to predict what the previous ones were unable to correctly predict. Repeat until reaching a stopping point parameter. The whole set of models is what's used to make predictions.\n",
    "- Boosting process:\n",
    "    - Randomly select a batch of data from training dataset without replacement to train \"weak learner.\"\n",
    "    - Randomly select a second batch of data from training dataset without replacement AND add around half of the samples that were misclassified from the previous model.\n",
    "    - Go back to the original training dataset and retrieve the data points in which the two models had differing classifications.\n",
    "    - Make predictions by combining the system of weak learners and takin the vote (classification) or avearge (regression.)\n",
    "    \n",
    "- Can be used both for regression and classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The AdaBoost (Adaptive Boosting) algorithm fits sequential weak classifiers, which are classifiers that are slightly better than random chance. These classifiers are usually tree-based models with a lower depth level. Adaboost actually uses the whole training dataset instead of sample. The data is weighted in each iteration of modeling to help it learn from the mistakes of the previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The weak learners in the AdaBoost algorithm are Decision Trees with one depth-level aka \"Decision Stumps.\" They literally only use one decision.\n",
    "\n",
    "- Each data point in the training data is assigned a weight. In the first model, every point has the same weight value which equal 1/number of values. \n",
    "\n",
    "- The first Decision Stump is fit on the whole data using weighted samples. Only works with binary clasification problems. The model outputs either a 1 or - 1, irregardless of the class labels in the target variable. \n",
    "\n",
    "- Error determined by the misclassification rate, which is 1 - accuracy score. Accuracy score of 0.71 means error rate of 0.29.\n",
    "\n",
    "- However error significantly changes when differents are introduced. \n",
    "\n",
    "- With weights, error = sum(w(i) * terror(i)) / sum(w). If terror is 1, then equals wrong prediction, 0 if correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost visually explained:\n",
    "\n",
    "![a](https://www.analyticsvidhya.com/wp-content/uploads/2015/11/bigd.png)\n",
    "\n",
    "Source: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box 1: Each data point has equal weighting is fit on a decision stump which is a vertical line.\n",
    "\n",
    "<br>\n",
    "\n",
    "Box 2: The three plus signs that were incorrectly classified in Box 1 have been enlarged (weighted) and the model has been retrained.\n",
    "\n",
    "<br>\n",
    "\n",
    "Box 3: Three minus signs have been given bigger weight values and the new model (horizontal line) has been fit to account for that.\n",
    "\n",
    "<br>\n",
    "\n",
    "Box 4: Combines the three Decision stump models, which vastly outperforms any of the three stumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually calculate weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of weights\n",
    "w = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "#List of actual values\n",
    "y = [1,  1, -1, 1, -1]\n",
    "#List of predictions\n",
    "p = [-1, 1, 1, 1, -1]\n",
    "#List or terrors\n",
    "t = [1, 0, 1, 0 , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular error rate calculation\n",
    "(1 + 0 + 1 + 0 + 0)/(1 + 1 + 1 + 1 + 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error calculation with weights (same product as above)\n",
    "e = (0.2 * 1 + 0.2 * 0 + 0.2 * 1 + 0.2 * 0 + 0.2 * 0)/ (0.2 + 0.2 + 0.2 + 0.2 + 0.2)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part we pass in the error rate through this function: 0.5 * log((1-e)/e)\n",
    "\n",
    "This gives a coefficient: a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import numpy\n",
    "import numpy as np\n",
    "\n",
    "a = 0.5 * np.log((1 - e)/ e)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this value to update our new weights.\n",
    "\n",
    "Formula is old weight value times the exponent of the negative value of a times prediction times actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First value\n",
    "\n",
    "w1 = 0.2 * np.exp(-a * 1 * -1)\n",
    "\n",
    "\n",
    "w2 = 0.2 * np.exp(-a * 1 * 1)\n",
    "\n",
    "\n",
    "w3 = 0.2 * np.exp(-a * 1 * -1)\n",
    "\n",
    "\n",
    "w4 = 0.2 * np.exp(-a * 1 * 1)\n",
    "\n",
    "\n",
    "w5 = 0.2 * np.exp(-a * 1 * 1)\n",
    "\n",
    "\n",
    "print (w1, w2, w3, w4, w5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights go up for wrong predictions and go down for correct ones.\n",
    "\n",
    "We're not finished yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we normalize the weight by diving each weight by the sum of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_sum = w1 + w2 + w3 + w4 + w5\n",
    "\n",
    "#New weights\n",
    "w1 = w1/(weight_sum)\n",
    "w2 = w2/(weight_sum)\n",
    "w3 = w3/(weight_sum)\n",
    "w4 = w4/(weight_sum)\n",
    "w5 = w5/(weight_sum)\n",
    "\n",
    "print (w1, w2, w3, w4, w5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are our new weights which we'll use in the next round of modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the follow - up model, a second Decision Stump model is trained using our new weights. The weights are used to determine the split in the decision tree. This process continues until we reach the n_estimators parameter we set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the weights for the mis-classified data points forces the models to train more heavily on the data it incorrectly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaBoost makes predictions by calculating the weighted average of the sequence of Decision Stumps. \n",
    "\n",
    "- When you pass in a new data point, the model predicts 1 or -1.\n",
    "\n",
    "- The weights of each model by each one's stage value. The prediction is derived from the sum of the of the weighted predictions. If sum > 0 then return the first class else return second class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Five model predictions\n",
    "preds = np.array([-1, -1, 1, -1, 1])\n",
    "preds.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without weighting the prediction would be -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([.2, .4, .8, .3, .9])\n",
    "\n",
    "sum(weights * preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction with weighted models equals 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Warnings</b>\n",
    "\n",
    "- Requires rich data noisy data by design can negatively influence model.\n",
    "- Same goes with outliers, the model will chase outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding AdaBoost\n",
    "\n",
    "1. Visualize the decision boundaries of AdaBoost\n",
    "\n",
    "2. Use AdaBoost on the spotify dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and visualize fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Generate fake data that is 400 x 2.\n",
    "data = make_classification(n_samples=400, n_features=2, n_informative=2, n_redundant=0, \n",
    "                    class_sep=.54, random_state = 8)\n",
    "\n",
    "df = pd.DataFrame(data[0], columns=[\"feature1\", \"feature2\"])\n",
    "#Add target variable to df \n",
    "df[\"target\"] = data[1]\n",
    "\n",
    "#Call scatter plot of feature1 vs feature2 with color-encoded target variable\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(11, 8))\n",
    "#Color encode target variable\n",
    "colors = df.target.map({0:\"b\", 1:\"r\"})\n",
    "plt.scatter(df.feature1, df.feature2, c = colors, s = 100, alpha=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "X = \n",
    "y = \n",
    "\n",
    "#Fit a Decision Tree model with max_depth = 2 on the data.\n",
    "\n",
    "dt = \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision boundary function\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, alpha=0.4, s = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed dt model, features and colors\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train AdaBoost model on the same data and visualize it\n",
    "\n",
    "#Intialize AdaBoost model with 20 estimators\n",
    "ada = \n",
    "\n",
    "#Fit model\n",
    "\n",
    "\n",
    "#Visualize model boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Using Spotify data to predict whether or not I will like a song? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Attributes </b>\n",
    "\n",
    "\n",
    "    Acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
    "    \n",
    "    Danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
    "\n",
    "    Instrumentalness: Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
    "    \n",
    "    Loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typical range between -60 and 0 db.\n",
    "    \n",
    "    Mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n",
    "\n",
    "    Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
    "    \n",
    "    Tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n",
    "\n",
    "    Energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
    "    \n",
    "More information here https://developer.spotify.com/web-api/get-audio-features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to my article about the project: https://opendatascience.com/blog/a-machine-learning-deep-dive-into-my-spotify-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import spotify data\n",
    "\n",
    "spotify = pd.read_csv(\"../../data/spotify_data.csv\", index_col=[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and contrast Decision Trees and AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Intialize AdaBoost with 300 estimators\n",
    "ada = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "X = \n",
    "\n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train test split with random state = 20\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit Ada boost model on training data and score it on testing\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation shows AdaBoost is a decent but not great model.\n",
    "\n",
    "Perhaps we chose the wrong estimator value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a validation curve to determine the best value for the estimator.\n",
    "\n",
    "<br>\n",
    "\n",
    "This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to time our code\n",
    "\n",
    "#Import time tool\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize time variable\n",
    "t = time()\n",
    "\n",
    "#Create list of estimator values\n",
    "estimators = range(50, 1050, 100)\n",
    "\n",
    "#Intialize cross validation scores list\n",
    "\n",
    "\n",
    "#Iterate over estimators values, fit models, and then append scores to cv_scores\n",
    "\n",
    "    \n",
    "    \n",
    "#Print difference in time\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#Plot estimators versus scores\n",
    "\n",
    "plt.figure(figsize= (8, 7))\n",
    "plt.plot(estimators, cv_scores, linewidth = 3)\n",
    "plt.xlabel(\"N Estimators\")\n",
    "plt.ylabel(\"Cross Validated Accuracy Scores\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive best estimator value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Evaluation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![s](http://www.dataschool.io/content/images/2015/01/confusion_matrix2.png)\n",
    "\n",
    "Source: Data Schoool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**True Positives (TP):** Number of correct positive predictions\n",
    "\n",
    "**True Negatives (TN):** Number of correct negative predictions\n",
    "\n",
    "**False Positives (FP):** Number incorrect positive predictions\n",
    "\n",
    "**False Negatives (FN):** Number of incorrect negative predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall aka sensitivity aka the True Positive Rate:** The number of correct positive predictions divided by number of positive instances\n",
    "\n",
    "**Precision:** The number of correct positive predictions divided by number of positive predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**False Positive Rate aka Fall Out:** The number of incorrect positive predictions divided by number of negative instances\n",
    "\n",
    "**True Negative Rate aka Specificity:** The number of correct negative predictions divided by number of negative instances "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula table:\n",
    "![a](http://www.chioka.in/wp-content/uploads/2013/08/Metrics-Table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix with metrics:\n",
    "\n",
    "![s](https://eus-www.sway-cdn.com/s/4YEmvTlyess2YF1M/images/VfcIF1yrYJrvLl?quality=1071&allowAnimation=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super confusion matrix:\n",
    "![q](https://image.ibb.co/bXkGxm/Screen_Shot_2017_11_28_at_12_03_48_PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about how these metrics can tell us more about the efficacy of a model as opposed to accuracy score.\n",
    "\n",
    "Is one metrics more useful than others? In which context would it make sense to evaluate a model based on FPR vs FNR?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train an Adaboost model with 50 estimators and make predictions using test set\n",
    "\n",
    "model = \n",
    "\n",
    "preds ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null accuracy of y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass the predictions and y_test into a confusion matrix\n",
    "cm = \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try calculating the TPR, TNR, FPR, and FNR rates manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TPR\n",
    "cm[1,1]/float(cm.sum(axis=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TNR\n",
    "cm[0,0]/float(cm.sum(axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FPR\n",
    "cm[0,1]/float(cm.sum(axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FNR\n",
    "cm[1,0]/float(cm.sum(axis=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were a spotify data scientist would you want a model that produces more false negatives or false positives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate precision and recall scores with sklearn\n",
    "ps = \n",
    "rs = \n",
    "\n",
    "print (\"The precision score is {:.2f} and the recall score is {:.2f}\".format(ps, rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validate with precision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![w](https://chrisalbon.com/images/machine_learning_flashcards/Receiver_Operating_Characteristic_print.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC (receiver operating characteristic) curve is a commonly used way to visualize the performance of a binary classifier.\n",
    "\n",
    "AUC (area under curve) is arguably the best way to summarize a model performance's in a single number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Derive probabilities of class 1 from the test set\n",
    "test_probs = \n",
    "#Pass in the test_probs variable and the true test labels aka y_test in the roc_curve function\n",
    "fpr, tpr, thres = \n",
    "#Outputs the fpr, tpr, for varying thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting False Positive Rates vs the True Positive Rates\n",
    "#Dotted line represents a useless model\n",
    "plt.figure(figsize=(13,10))\n",
    "plt.plot(fpr, tpr, linewidth=8)\n",
    "#Line of randomness\n",
    "plt.plot([0,1], [0,1], \"--\", alpha=.7)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you rate this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Caculate the area under the curve score using roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validated roc_auc score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting threshold vs FPR/TPR on the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(thres, fpr, linewidth=5, label = \"FPR Line\")\n",
    "plt.plot(thres, tpr, linewidth=5, label = \"TPR line\")\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"False Positive Rate\")\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see here? Why are is there a negative correlation in both lines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Sklearn tools\n",
    "\n",
    "Overview:\n",
    "\n",
    "- Grid search\n",
    "- Pipelining\n",
    "- Imputation\n",
    "- Feature unions\n",
    "- Feature selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch\n",
    "\n",
    "Algorithm that tests every combination of model parameters to find the best one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use GridSearch to find the best K value for a KNN model and Spotify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize parameter grid\n",
    "\n",
    "#Range of neighbors to test\n",
    "neighbors_range = \n",
    "\n",
    "#Dictionary of parameter values \n",
    "param_grid_knn = \n",
    "\n",
    "\n",
    "param_grid_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "grid_knn =\n",
    "\n",
    "#Fit grid on data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whats the best cross validated accuracy score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the best parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple technique gives us the best K value.\n",
    "\n",
    "We can use the best model from grid_knn to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input \n",
    "x = [[0.2, .15, 0.68, 0.05, 0.328]]\n",
    "\n",
    "#Make prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick exercise:\n",
    "<br>\n",
    "Use grid search to determine the best depth value in a decision tree. Use depths from 2 - 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Range of neighbors to test\n",
    "depths_range = \n",
    "\n",
    "#Dictionary of parameter values \n",
    "param_grid_dt = {}\n",
    "\n",
    "\n",
    "param_grid_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CV in GridSearchCV stands for cross validation which means we have to set a cv and scoring value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "grid_dt = \n",
    "\n",
    "#Fit grid on data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score for DT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameter for DT model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our grids have been one-dimensional, now let's try using multiple dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Param grid with test different split criteria as well.\n",
    "param_grid_dt = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": depths_range}\n",
    "\n",
    "param_grid_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's going to cross-validate every combination between the criterion parameters and depth parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "grid_dt = GridSearchCV(estimator = DecisionTreeClassifier(), \n",
    "                        param_grid = param_grid_dt, cv = 5, scoring = \"accuracy\")\n",
    "#Fit grid on data\n",
    "grid_dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameter\n",
    "\n",
    "grid_dt.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score\n",
    "\n",
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many models did this grid search function conduct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add in some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid_dt[\"min_samples_split\"] =[2, 10, 20]\n",
    "param_grid_dt[\"max_features\"] = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "grid_dt = \n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously grid search takes a long time and in some case can cause memory errors. This is where RandomizedSearchCV comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import \n",
    "from sklearn.grid_search import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions just like GridSearchCV, except we have to choose a value n_iter which is the random number of combinations we testing and set param_distributions instead of param_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize RandomizedSearchCV grid with n_iter = 20\n",
    "grid_dt = \n",
    "\n",
    "#Time the code \n",
    "\n",
    "t = time()\n",
    "\n",
    "#Fit grid on data\n",
    "\n",
    "\n",
    "#Print time difference\n",
    "\n",
    "print (time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced run time by a huge percentage!\n",
    "\n",
    "But now let's see if we sacrificed performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check best score\n",
    "\n",
    "grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines\n",
    "\n",
    "Let's go back to using the KNN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we need to scale our data for the KNN algorithm right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data and fit it a Grid search function it.\n",
    "\n",
    "#Intialize scalar\n",
    "scale = \n",
    "\n",
    "#Fit and transform scaler on the data\n",
    "Xs = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Grid\n",
    "\n",
    "grid_knn_s = GridSearchCV(estimator = KNeighborsClassifier(), \n",
    "                        param_grid = param_grid_knn, cv = 5, scoring = \"accuracy\")\n",
    "\n",
    "#Fit grid on scaled data\n",
    "\n",
    "grid_knn_s.fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best score\n",
    "grid_knn_s.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best K value\n",
    "grid_knn_s.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make prediction\n",
    "\n",
    "#First transform predict using scaler\n",
    "\n",
    "xs = \n",
    "\n",
    "#Pass in xs to grid model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass scaler and knn classifier objects into make_pipeline function\n",
    "pipe = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new param_grid\n",
    "neighbors_range = range(2, 21)\n",
    "param_grid_knn = {}\n",
    "param_grid_knn[\"kneighborsclassifier__n_neighbors\"] = neighbors_range\n",
    "param_grid_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pass in pipe into GridSearchCV function, \n",
    "grid_knn_pipe = GridSearchCV(pipe, param_grid=param_grid_knn, cv=5, scoring='accuracy')\n",
    "\n",
    "#Fit on original versions of data\n",
    "grid_knn_pipe.fit(X, y)\n",
    "\n",
    "#Best scores and params\n",
    "print grid_knn_pipe.best_score_, grid_knn_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also pass in the pipe object into a cross_val_score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cross-validation process using Pipeline\n",
    "pipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n",
    "cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class exercise: Use grid search to model the 2016 Democratic primary data with the person who is next to you at your table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in data files\n",
    "primary = pd.read_csv(\"../data/primary_data//primary_results.csv\")\n",
    "county = pd.read_csv(\"../data/primary_data/county_facts.csv\")\n",
    "county_dict = pd.read_csv(\"../data/primary_data/county_facts_dictionary.csv\")\n",
    "\n",
    "subset_col_index = [0,3,5,9,10,12,18,20,23,25,33,34,53]\n",
    "\n",
    "county = county.iloc[:,subset_col_index].copy()\n",
    "\n",
    "subset_cols = [\"fips\",\"population\", \"pop_change\", \"senior_pop_per\", \"female_pop_per\", \"black_pop_per\",\n",
    "               \"white_pop_per\", \"foreign_pop_per\", \"college_degree_pop_\", \"commute_time\", \"median_income\",\n",
    "               \"poverty_rate\", \"pop_density\"]\n",
    "\n",
    "col_dict = dict(zip(county.columns, subset_cols))\n",
    "#Use dictionary to rename the columns\n",
    "county.rename(columns=col_dict, inplace=True)\n",
    "primary.dropna(inplace=True)\n",
    "bern = primary[primary.candidate== \"Bernie Sanders\"]\n",
    "hill = primary[primary.candidate== \"Hillary Clinton\"]\n",
    "bern = bern[[\"fips\", \"candidate\", \"votes\"]]\n",
    "dem = pd.merge(hill, bern, on=\"fips\")\n",
    "dem.rename(columns={\"votes_x\":\"clinton_votes\", \"votes_y\":\"sanders_votes\"}, inplace=True)\n",
    "dem[\"winner\"] = dem.clinton_votes - dem.sanders_votes\n",
    "def vote_winner(x):\n",
    "    if x >0:\n",
    "        return \"H\"\n",
    "    elif x == 0:\n",
    "        return \"TIE\"\n",
    "    else:\n",
    "        return \"B\"\n",
    "    \n",
    "dem[\"winner\"] = dem.winner.apply(vote_winner)\n",
    "\n",
    "dem = dem[dem.winner!= \"TIE\"]\n",
    "dem = dem[[\"fips\", \"winner\"]]\n",
    "df = pd.merge(county, dem, on=\"fips\")\n",
    "df.set_index(\"fips\", inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a pipeline using regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in boston dataset\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "df = pd.DataFrame(boston[\"data\"])\n",
    "df.columns = boston[\"feature_names\"]\n",
    "df[\"MEDV\"] = boston[\"target\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston[\"DESCR\"].split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign X and y\n",
    "\n",
    "X = df.drop(\"MEDV\", axis =1)\n",
    "y = df.MEDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a Pipeline Class instead of function to establish pipeline\n",
    "pipe_poly = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a few features from X\n",
    "XX = X[[\"RM\", \"DIS\", \"NOX\", \"CRIM\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize range values for poly\n",
    "poly_range = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "#Intialize grid dictionary\n",
    "param_grid_poly = {}\n",
    "\n",
    "#Input grid values\n",
    "param_grid_poly[\"polynomialfeatures__degree\"] = poly_range\n",
    "\n",
    "#Establish the grid\n",
    "poly_grid = GridSearchCV(pipe_poly, \n",
    "                         param_grid = param_grid_poly, cv=5, \n",
    "                         scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on data\n",
    "\n",
    "poly_grid.fit(XX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized Grid Search with Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_poly = Pipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n",
    "                            ('ridgeregression', Ridge())]) \n",
    "\n",
    "param_grid_ridge = {'polynomialfeatures__degree': [1, 2, 3, 4, 5],\n",
    "              'ridgeregression__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_ridge = RandomizedSearchCV(pipe_poly, param_distributions=param_grid_ridge, \n",
    "                                n_iter = 5 , cv = 5, scoring='neg_mean_squared_error')\n",
    "grid_ridge.fit(XX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (grid_ridge.best_params_, grid_ridge.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "<b> Boosting: </b>\n",
    "\n",
    "- https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/\n",
    "\n",
    "- https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/\n",
    "\n",
    "- http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\n",
    "\n",
    "- https://www.youtube.com/watch?v=Rm6s6gmLTdg&list=PLaslQpv_LmSKxSCBPdKWEI7lLHrTCeewl\n",
    "\n",
    "- https://www.coursera.org/learn/practical-machine-learning/lecture/9mGzA/boosting\n",
    "\n",
    "<b> Grid Search and Pipelines </b>\n",
    "\n",
    "- https://chrisalbon.com/machine-learning/cross_validation_parameter_tuning_grid_search.html\n",
    "- https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/\n",
    "- https://www.youtube.com/watch?v=Gol_qOgRqfA\n",
    "- https://chrisalbon.com/machine-learning/pipelines_with_parameter_optimization.html\n",
    "- https://chrisalbon.com/machine-learning/hyperparameter_tuning_using_random_search.html\n",
    "- https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/\n",
    "- https://www.civisanalytics.com/blog/workflows-in-python-using-pipeline-and-gridsearchcv-for-more-compact-and-comprehensive-code/\n",
    "\n",
    "\n",
    "Evalution:\n",
    "- http://www.dataschool.io/roc-curves-and-auc-explained/\n",
    "- http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In class work.\n",
    "\n",
    "Choose one of the following dataset to work on modeling for the rest of class using the new models and tools we've learned in the past couple weeks.\n",
    "\n",
    "<br>\n",
    "Spotify, Dem Primary, KC Housing, Movie Metadata, HR Employee, Breast Cancer, Default, Mushrooms, Red win quality, Zillow starter, or Pokemon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
