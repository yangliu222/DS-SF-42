{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees, Bias vs Variance, and Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "\n",
    "- **Decision Trees** algorithm: how it works, how to use it, and how to interpret it.\n",
    "    - Parameters\n",
    "    - Visualizing decision trees\n",
    "    - Validation and learning curves\n",
    "- **Bias vs Variance** aka the eternal data science dilemma. We'll learn to find the right balance between the two.\n",
    "- **Cross validation**. Testing how well our model does on data it hasn't seen before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install graphviz with `pip install graphviz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score, train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the most popular and oldest machine learning models.\n",
    "- Both a classification and regression supervised model.\n",
    "- Intuitive. Very obvious why it's called \"Decision Trees.\" \n",
    "- Foundation is asking a series of questions designed to zero-in on the classification. Sound familiar? (Hint: 20 questions.)\n",
    "- Gateway model to more complex models such as Random Forests and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a Decision Tree works\n",
    "What makes Decision Trees great is that you can see the process in how it determines a classification.\n",
    "<br><br>\n",
    "Below is a decision tree that models data from the 2008 Democratic primary. It is predicting whether or not a county voted for Hillary Clinton or Barack Obama based on demographics and other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stuff](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:**\n",
    "\n",
    "- What are the observations? How many observations are there?\n",
    "- What is the response variable?\n",
    "- What are the features?\n",
    "- How are the predictions made?\n",
    "- What is the most predictive feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first split is \"County > 20% Black Population\". In this example, when a splitting rule is <b>True</b>, the model follows the right branch and when the splitting rule is <b>False</b>, it follows the left branch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which candidate would the model predict winning this county?\n",
    "15% Black, 82% HS graduation rate, located in Michigan, 60% earned less than $30,000\n",
    "<br><br>\n",
    "Whats the probability for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of questions indicates the depth of the model, which measures the complexitiy of a model. \n",
    "- Model splits until every observation that satisfies a series of conditions is of the same class or until the model reaches the maximum depth.\n",
    "- Setting a maximum depth is known as \"pruning\" and not setting one or choosing too big of one can lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Why did this model make those decisions? Why is fewer than 20% black the first decision? Why 20% instead of 16 or 25?</b>\n",
    "- At each node, we choose the decision that provides the best split, this is known as maximixing the information gain.\n",
    "- The point where the split is made is determined by which point yields the lowest gini coefficient which measures the quality of a split (0 means perfect split.)\n",
    "- Order of decisions is based on feature importance. The first decision is made on the model's most important feature. Very valuable method for feature selection and engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to use sklearn to make some fake data.\n",
    "\n",
    "#Generate fake data that is 400 x 2.\n",
    "data = make_classification(n_samples=400, n_features=2, n_informative=2, n_redundant=0, \n",
    "                    class_sep=.74, random_state = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0] is the features\n",
    "# data[1] is the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to pandas data frame\n",
    "\n",
    "#Add target variable to df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot data with its color-encoded labels. Try to imagine how a decision tree would classify this data. Remember decision trees boundaries are vertical or horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call scatter plot of feature1 vs feature2 with color-encoded target variable\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "#Color encode target variable\n",
    "colors = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start thinking about how a model would classify red vs blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see what the model says.\n",
    "<br><br>\n",
    "We are going to use the data to <b>train</b> or <b>fit</b> the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign features to variable X\n",
    "X = \n",
    "#Assign target column to variable y\n",
    "y = \n",
    "\n",
    "#Fit a Decision Tree model with 2 depth on the data.\n",
    "\n",
    "#1. Intialize the model, set max_depth = 2\n",
    "model = \n",
    "#2. Fit/train the model. \n",
    "\n",
    "\n",
    "#The model specifications \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call predict on model and pass new data\n",
    "\n",
    "point = [[0,0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call predict_proba()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how good our model by scoring the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call .score() on model object and pass in the features and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use a plotting function to visualize the model.\n",
    "\n",
    "What do you think that means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain model but this time with depth 3\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the model decision boundary function\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, alpha=0.4, s = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass in model and data into function.\n",
    "#Input a pretrained model, then features, then color-encoded target variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! These are the decision boundaries of decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples of decision tree boundaries.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Four blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example1](https://image.ibb.co/m9EjYG/decisionblobs.png)\n",
    "\n",
    "\n",
    "\n",
    "Credit: [Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how different decision tree models with varying depth values interpret the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blobs](https://image.ibb.co/hRE7nb/decisionblobboundaries.png)\n",
    "Credit: Jake VanderPlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris dataset/tree visualization\n",
    "\n",
    "We're going to train a decision tree model on the iris dataset and then use visualize the actual decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the iris dataset from seaborn\n",
    "iris = \n",
    "\n",
    "#Look at first five rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the classes of the target variable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time for some quick EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the data by species and derive the average values of each attribute in each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us about the differences and similarities of each species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a pairplot to visualize the dataset and use the species column to color encode the plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again what does this tell us? How can we use this plot to our advantage when we create our machine learning model? What are the vertical and horizontal boundaries between the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign iris features to X and species to y\n",
    "\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit decision tree model on iris data with max depth set to 3\n",
    "\n",
    "iris_model = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the decision tree graph viz object. We have to export and the re-import it\n",
    "export_graphviz(iris_model, out_file='iris.dot', \n",
    "                    feature_names=X.columns, \n",
    "                    class_names=y.unique())\n",
    "with open(\"iris.dot\") as f: \n",
    "        dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Parameters\n",
    "\n",
    "We've learned about max_depth, now let's look at some of the other parameters we can tune to improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Data Aspirant](http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/)\n",
    "\n",
    "**criterion:** It defines the function to measure the quality of a split. Sklearn supports “gini” criteria for Gini Index & “entropy” for Information Gain. By default, it takes “gini” value. (Further explanation on gini at the end of this notebook)\n",
    "\n",
    "**max_features:** It defines the no. of features to consider when looking for the best split. We can input integer, float, string & None value.\n",
    "    - If an integer is inputted then it considers that value as max features at each split.\n",
    "    - If float value is taken then it shows the percentage of features at each split.\n",
    "    \n",
    "**max_depth:** The max_depth parameter denotes maximum depth of the tree. It can take any integer value or None. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. By default, it takes “None” value.\n",
    "\n",
    "**min_samples_split:** This tells above the minimum no. of samples reqd. to split an internal node. If an integer value is taken then consider min_samples_split as the minimum no. If float, then it shows percentage. By default, it takes “2” value.\n",
    "\n",
    "**min_samples_leaf:** The minimum number of samples required to be at a leaf node. If an integer value is taken then consider min_samples_leaf as the minimum no. If float, then it shows percentage. By default, it takes “1” value.\n",
    "\n",
    "**max_leaf_nodes:** It defines the maximum number of possible leaf nodes. If None then it takes an unlimited number of leaf nodes. By default, it takes “None” value.\n",
    "\n",
    "**min_impurity_split:** It defines the threshold for early stopping tree growth. A node will split if its impurity is above the threshold otherwise it is a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias vs Variance\n",
    "<br><br>\n",
    "<b>Bias:</b> The simplifying assumptions made by the model to make the target function easier to approximate.\n",
    "\n",
    "<b>Variance:</b> The amount that the estimate of the target function will change given different training data.\n",
    "\n",
    "From: https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/\n",
    "<br><br>\n",
    "[Legendary data science blog post](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "\n",
    "<b>Bias error:</b> The difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Bias measures how far off in general these models' predictions are from the correct value.\n",
    "\n",
    "<b>Variance error:</b> The error due to variance is taken as the variability of a model prediction for a given data point. Imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphic illustration of bias vs variance:\n",
    "\n",
    "![b v v](https://i.stack.imgur.com/r7QFy.png)\n",
    "\n",
    "Credit: Scott Fortmann-Roe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see here? How would you interpret this graphic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's bring back to the model plotting function for the purpose of visualizing an overfit model against a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate fake data that is 400 x 2.\n",
    "data = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, \n",
    "                    class_sep=.20, random_state = 34)\n",
    "#Assign features to XX\n",
    "X = \n",
    "#Assign target variable to yy\n",
    "y = \n",
    "\n",
    "#Set size\n",
    "plt.figure(figsize=(11, 8))\n",
    "\n",
    "#Plot features and use to yy to color-encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train test split with 25% test size\n",
    "\n",
    "#fit default model on the training set\n",
    "model = \n",
    "\n",
    "\n",
    "#Evaluate model on training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Perfect model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or is it????!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the y_test variable into list of colors\n",
    "colors = list(map(lambda x: \"red\" if x == 1 else \"blue\", y_test.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the model and the testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that look to you? Where in the plot is the model overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on the testing set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this mean? Why the huge gulf between the training and testing scores? What is the influence of max_depth?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this training/testing process again to see if we get different scores\n",
    "\n",
    "<br><br>\n",
    "Run this code several times and observe the changes in the testing score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Different train/test split but with no random_state set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = .25)\n",
    "\n",
    "#Fit model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "training_score = model.score(X_train, y_train)\n",
    "\n",
    "print (\"The training score is {:.3f} percent\".format(training_score*100))\n",
    "\n",
    "testing_score = model.score(X_test, y_test)\n",
    "print (\"The testing score is {:.3f}\".format(testing_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make this a for loop\n",
    "\n",
    "#Intialize list that we'll use for our testing scores\n",
    "testscorelist = []\n",
    "\n",
    "#Iterate over range 10\n",
    "for i in range(10):\n",
    "    #Split data, fit model, test data, append testing score to testscorelist\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X ,y, test_size = .25)\n",
    "    #Fit model\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    #Score on testing set\n",
    "    ts = model.score(X_test, y_test)\n",
    "    testscorelist.append(ts)\n",
    "\n",
    "testscorelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "<br><br>\n",
    "\n",
    "\"Cross-validation, sometimes called rotation estimation, is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set.\" \n",
    "\n",
    "https://en.wikipedia.org/wiki/Cross-validation_(statistics)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "\n",
    "<b>K-Fold Cross Validation</b>\n",
    "![Image](https://i.stack.imgur.com/1fXzJ.png)\n",
    "\n",
    "<br><br>\n",
    "\"[In K Fold cross validation](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f), the data is divided into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set/ validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total effectiveness of our model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchanging the training and test sets also adds to the effectiveness of this method. As a general rule and empirical evidence, K = 5 or 10 is generally preferred, but nothing’s fixed and it can take any value.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use cross_val_score function to perform KFold cross validation five times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call cross_val_score, input empty DT model, X, y, set cv = 5 and scoring = accuracy\n",
    "cv_scores = \n",
    "\n",
    "#Call cv_scores\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see theres a degree of variance in the output, which makes deriving the mean crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whats the average score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Class exercise:</b>\n",
    "\n",
    "Test to see the relationship between max_depth and the average cv_score. What happens when you increase or decrease max_depth. \n",
    "\n",
    "Whats you're done playing around with that, then make a line plot of depth values from 1 - 20 and the average cross validated score for each corresponding depth value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best depth value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart above is very similar to a validation curve, which we'll get more into later in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model with the best depth value and evaluate it on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign best depth value to depth\n",
    "depth = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test with random state = 4 and test size = .25\n",
    "X_train, X_test, y_train, y_test = \n",
    "\n",
    "\n",
    "#Fit model with best depth value\n",
    "model = \n",
    "\n",
    "\n",
    "#Score model on test set\n",
    "testscore = \n",
    "\n",
    "print (\"The test score is {:.3f} percent\".format(testscore*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does that compare to the null accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subtract null accuracy from testscore\n",
    "null_acc = \n",
    "\n",
    "testscore - null_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this good or bad? How do you interpret this result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best depth is one that is not too small but not too large.\n",
    "\n",
    "We need to find the depth that strikes the right balance between <b>bias</b> and <b>variance</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation and Learning Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Depicting bias vs variance with validation and learning curves</b>\n",
    "\n",
    "Validation Curve:\n",
    "\n",
    "![Lc](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n",
    "\n",
    "<br><br>\n",
    "Learning Curve:\n",
    "![lc](https://chrisalbon.com/images/machine_learning_flashcards/Learning_Curve_print.png)\n",
    "<br><br>\n",
    "[\"Graph that compares the performance of a model on training and testing data over a varying number of training instances\"](http://www.ritchieng.com/machinelearning-learning-curve/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go over how to make validation and learning curve plots in sklearn, let's import the titanic dataset and clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in dataset\n",
    "path = \"../../data/titanic.csv\"\n",
    "\n",
    "titanic = pd.read_csv(path)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data dictionary:</b>\n",
    "\n",
    "PassengerID: A column added by Kaggle to identify each row and make submissions easier\n",
    "\n",
    "Survived: Whether the passenger survived or not and the value we are predicting (0=No, 1=Yes)\n",
    "\n",
    "Pclass:\tThe class of the ticket the passenger purchased (1=1st, 2=2nd, 3=3rd)\n",
    "\n",
    "Sex: The passenger’s sex\n",
    "\n",
    "Age: The passenger’s age in years\n",
    "\n",
    "SibSp: The number of siblings or spouses the passenger had aboard the Titanic\n",
    "\n",
    "Parch: The number of parents or children the passenger had aboard the Titanic\n",
    "\n",
    "Ticket: The passenger’s ticket number\n",
    "\n",
    "Fare: The fare the passenger paid\n",
    "\n",
    "Cabin: The passenger’s cabin number\n",
    "\n",
    "Embarked— The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean the data\n",
    "\n",
    "def titanic_clean(df):\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.set_index(\"passengerid\", inplace=True)\n",
    "    df.age.fillna(df.age.median(), inplace = True)\n",
    "    df.drop(\"cabin\", axis =1, inplace= True)\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop([\"name\", \"ticket\"], axis =1, inplace= True)\n",
    "    gender_dict = {\"male\":0, \"female\":1}\n",
    "    df[\"sex\"] = df.sex.map(gender_dict)\n",
    "    df_emb_dums = pd.get_dummies(df.embarked, prefix = \"emb\", drop_first=True)\n",
    "    df.drop(\"embarked\", axis =1 , inplace=True)\n",
    "    df = pd.concat([df, df_emb_dums], axis =1 )\n",
    "    return df\n",
    "\n",
    "#pass data through function and reassign titanic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First extract features and target variables\n",
    "\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Null accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our null accuracy is 61.75%. That means we have to create a model that classifies the data at a better rate than 61.75%.\n",
    "\n",
    "If we didn't build a model and just said everyone died, then we'd be 61.75% without even going through the trouble of building a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from [www.chrisalbon.com](www.chrisalbon.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create range of values for parameter\n",
    "depth_range = np.arange(1, 25)\n",
    "\n",
    "# Calculate accuracy on training and test set using range of parameter values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean for training set scores\n",
    "train_mean = \n",
    "\n",
    "# Calculate mean for test set scores\n",
    "test_mean = \n",
    "\n",
    "\n",
    "#Set size\n",
    "plt.figure(figsize=(11, 7))\n",
    "\n",
    "# Plot mean accuracy scores for training and test sets\n",
    "\n",
    "\n",
    "\n",
    "# Create plot\n",
    "\n",
    "plt.title(\"Validation Curve on titanic dataset\", fontsize = \"x-large\")\n",
    "plt.xlabel(\"Depth Value\", fontsize = \"x-large\")\n",
    "plt.ylabel(\"Accuracy Score\",  fontsize = \"x-large\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"best\", fontsize = \"x-large\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this graph mean? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign best depth value to depth\n",
    "\n",
    "depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create values for train_sizes, train_scores, and test_scores\n",
    "train_sizes, train_scores, test_scores = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create means of training set scores\n",
    "train_mean = \n",
    "\n",
    "# Create means of test set scores\n",
    "test_mean = \n",
    "\n",
    "#Set size\n",
    "plt.figure(figsize=(11, 7))\n",
    "\n",
    "# Draw lines\n",
    "\n",
    "\n",
    "# Create plot\n",
    "\n",
    "plt.title(\"Learning Curve on Titanic Dataset\", fontsize = \"x-large\")\n",
    "plt.xlabel(\"Training Set Size\", fontsize = \"x-large\")\n",
    "plt.ylabel(\"Accuracy Score\",  fontsize = \"x-large\")\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"best\", fontsize = \"x-large\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are your initial observations of this graph? What does it say about your model and your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus!!\n",
    "<br><br>\n",
    "Let's see the most important features and visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit DT model on X and y with max_depth 4\n",
    "\n",
    "dt = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call .feature_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets put that in a dataframe\n",
    "fi = \n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the decision tree graph viz object. We have to export and the re-import it\n",
    "export_graphviz(dt, out_file='titanic.dot', \n",
    "                    feature_names=X.columns, \n",
    "                    class_names=[\"dead\", \"alive\"])\n",
    "with open(\"titanic.dot\") as f: \n",
    "        dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "Bias vs variance:\n",
    "\n",
    "https://ml.berkeley.edu/blog/2017/07/13/tutorial-4/\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/\n",
    "\n",
    "http://www.machinelearningtutorial.net/2017/01/26/the-bias-variance-tradeoff/\n",
    "\n",
    "https://followthedata.wordpress.com/2012/06/02/practical-advice-for-machine-learning-bias-variance/\n",
    "\n",
    "\n",
    "<br><br>\n",
    "Cross validation:\n",
    "\n",
    "https://stats.stackexchange.com/questions/1826/cross-validation-in-plain-english\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2015/11/improve-model-performance-cross-validation-in-python-r/\n",
    "\n",
    "https://www.openml.org/a/estimation-procedures/1\n",
    "\n",
    "<br><br>\n",
    "Learning and validation curves:\n",
    "\n",
    "->->**https://www.dataquest.io/blog/learning-curves-machine-learning/**\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html\n",
    "\n",
    "<br><br>\n",
    "Titanic dataset projects:\n",
    "\n",
    "https://www.kaggle.com/maielld1/titanic-dataquest-tutorial\n",
    "\n",
    "https://github.com/agconti/kaggle-titanic\n",
    "\n",
    "https://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class work \n",
    "\n",
    "For the rest of class, work on improving your model as much as possible. \n",
    "\n",
    "- See what happens when you drop different features\n",
    "- Try different combinations of them\n",
    "- Try making pclass into a dummy variable instead of a continuous one\n",
    "- Make predictions of \"fake passengers\". Input a bunch of features to see what happens.\n",
    "- You can also try these techniques on other datasets as well.\n",
    "\n",
    "You're also welcome to spend the rest of the class checking out the resources I provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
